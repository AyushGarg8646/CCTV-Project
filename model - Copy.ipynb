{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1af54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !pip install ultralytics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6383fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import cv2 as cv\n",
    "import time\n",
    "# import fastreid.modeling.heads \n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "model2=YOLO(\"yolo11n.pt\")\n",
    "model3=YOLO(\"yolov8m.pt\")\n",
    "model4=YOLO(\"yolov8x.pt\")\n",
    "\n",
    "\n",
    "# Your other imports like numpy, pandas, etc. go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f283c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859a7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f562ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image # <--- Import the Image class from Pillow\n",
    "\n",
    "# --- 1. Load the image using OpenCV ---\n",
    "\n",
    "def transform(img):\n",
    "# --- 2. Define the Transformation Pipeline (no changes here) ---\n",
    "    image_height = 256\n",
    "    image_width = 128\n",
    "    pixel_mean = [0.485, 0.456, 0.406]\n",
    "    pixel_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.Resize((image_height, image_width)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=pixel_mean, std=pixel_std)\n",
    "    ])\n",
    "\n",
    "    # --- 3. Apply the Transformations ---\n",
    "\n",
    "    # a. Convert color from BGR (OpenCV) to RGB\n",
    "    rgb_crop = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "    # b. Convert the NumPy array to a PIL Image  <--- THIS IS THE FIX\n",
    "    pil_image = Image.fromarray(rgb_crop)\n",
    "\n",
    "    # c. Apply the preprocessing pipeline to the PIL Image\n",
    "    tensor = transform(pil_image)\n",
    "\n",
    "    # d. Add the batch dimension\n",
    "    input_batch = tensor.unsqueeze(0)\n",
    "\n",
    "    # e. Move to the correct device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    input_batch = input_batch.to(device)\n",
    "    \n",
    "    \n",
    "    return input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0c0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully. Using device: cuda\n",
      "Model built successfully.\n",
      "Model set to evaluation mode.\n",
      "\n",
      "--- Manually Correcting and Loading Weights ---\n",
      "❌ Error! Still some mismatches after correction.\n",
      "Missing keys in model: []\n",
      "Unexpected keys in file: ['pixel_mean', 'pixel_std']\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--- Model Architecture ---\n",
      "Baseline(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): IBN(\n",
      "          (IN): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (BN): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (heads): EmbeddingHead(\n",
      "    (pool_layer): GlobalAvgPool(output_size=1)\n",
      "    (bottleneck): Sequential(\n",
      "      (0): BatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (cls_layer): Linear(num_classes=751, scale=1, margin=0.0)\n",
      "  )\n",
      ")\n",
      "--------------------------\n",
      "\n",
      "FastReID model is ready for feature extraction.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# It is assumed that this script is run from a directory that contains\n",
    "# the cloned 'fast-reid' repository.\n",
    "# Add the fast-reid repository to the Python path.\n",
    "import sys\n",
    "sys.path.append('fast-reid')\n",
    "\n",
    "from fastreid.config import get_cfg\n",
    "from fastreid.modeling import build_model\n",
    "from fastreid.utils.checkpoint import Checkpointer\n",
    "import fastreid.modeling.heads \n",
    "import fastreid.modeling.backbones\n",
    "def setup_cfg(config_file, weights_file):\n",
    "    \"\"\"\n",
    "    Creates a FastReID config object and merges it with the specified\n",
    "    configuration file and weights.\n",
    "    \"\"\"\n",
    "    # 1. Get a copy of the default configuration\n",
    "    cfg = get_cfg()\n",
    "\n",
    "    # 2. Add project-specific configurations if any\n",
    "    # (Not needed for this baseline model)\n",
    "\n",
    "    # 3. Merge from the specified YAML configuration file\n",
    "    # This file contains the model architecture and dataset specifics.\n",
    "    cfg.merge_from_file(config_file)\n",
    "\n",
    "    # 4. Set the path to the pre-trained model weights.\n",
    "    # This is a critical step for loading the desired model state.\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "\n",
    "    # 5. Set the device to use for inference.\n",
    "    # Use 'cuda' for GPU or 'cpu' for CPU.\n",
    "    cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    cfg.MODEL.HEADS.NUM_CLASSES = 751\n",
    "    # 6. Freeze the configuration.\n",
    "    # This makes the config object immutable and prevents accidental changes.\n",
    "    cfg.freeze()\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load the model and print its architecture.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    # Define paths relative to the project structure.\n",
    "    project_root = Path('.')\n",
    "    fastreid_repo_path = project_root / 'fast-reid'\n",
    "    \n",
    "    # Path to the configuration YAML file.\n",
    "    config_file_path = fastreid_repo_path / 'configs/Market1501/bagtricks_R101-ibn.yml'\n",
    "    \n",
    "    # Path to the pre-trained weights file.\n",
    "    weights_file_path = project_root / \"C:/pooo/models/market_bot_R101-ibn.pth\"\n",
    "\n",
    "    # Verify that the necessary files exist.\n",
    "    if not config_file_path.is_file():\n",
    "        print(f\"Error: Config file not found at {config_file_path}\")\n",
    "        return\n",
    "    if not weights_file_path.is_file():\n",
    "        print(f\"Error: Weights file not found at {weights_file_path}\")\n",
    "        print(\"Please download 'market_bot_R50.pth' and place it in the 'models/' directory.\")\n",
    "        return\n",
    "\n",
    "    # --- Model Loading ---\n",
    "    # 1. Setup the configuration object.\n",
    "    cfg = setup_cfg(str(config_file_path), str(weights_file_path))\n",
    "    print(f\"Configuration loaded successfully. Using device: {cfg.MODEL.DEVICE}\")\n",
    "\n",
    "    # 2. Build the model.\n",
    "    # The `build_model` function reads the configuration and constructs the\n",
    "    # corresponding model architecture. It also automatically loads the weights\n",
    "    # specified in `cfg.MODEL.WEIGHTS`.\n",
    "    model = build_model(cfg)\n",
    "    \n",
    "    print(\"Model built successfully.\")\n",
    "\n",
    "    # The Checkpointer is used internally by `build_model` but can also be\n",
    "    # used explicitly if needed. For this use case, `build_model` handles it.\n",
    "    # Checkpointer(model).load(cfg.MODEL.WEIGHTS) # This is done inside build_model\n",
    "\n",
    "    # 3. Set the model to evaluation mode.\n",
    "    # This is crucial for consistent results during inference as it disables\n",
    "    # layers like Dropout and uses running statistics for Batch Normalization.\n",
    "    model.eval()\n",
    "    print(\"Model set to evaluation mode.\")\n",
    "\n",
    "    # --- Robust Weight Verification ---\n",
    "   # --- Manual Weight Correction and Loading ---\n",
    "    print(\"\\n--- Manually Correcting and Loading Weights ---\")\n",
    "    \n",
    "    device = torch.device(cfg.MODEL.DEVICE)\n",
    "    weights_from_file = torch.load(weights_file_path, map_location=device)\n",
    "    \n",
    "    # Check if weights are nested under a 'model' key\n",
    "    if 'model' in weights_from_file:\n",
    "        weights_from_file = weights_from_file['model']\n",
    "\n",
    "    # Create a new dictionary to hold the corrected weights\n",
    "    from collections import OrderedDict\n",
    "    corrected_weights = OrderedDict()\n",
    "    \n",
    "    # Iterate through the keys from the file and rename them\n",
    "    for k, v in weights_from_file.items():\n",
    "        # --- THIS IS THE FIX ---\n",
    "        if k.startswith('module.'):\n",
    "             # Remove 'module.' prefix if it exists\n",
    "            new_key = k[7:]\n",
    "        else:\n",
    "            new_key = k\n",
    "        \n",
    "        # Rename the head layers\n",
    "        if new_key == 'heads.classifier.weight':\n",
    "            new_key = 'heads.weight'\n",
    "        elif 'heads.bnneck.' in new_key:\n",
    "            new_key = new_key.replace('heads.bnneck.', 'heads.bottleneck.0.')\n",
    "        \n",
    "        corrected_weights[new_key] = v\n",
    "        # --------------------\n",
    "\n",
    "    # Load the corrected weights into the model.\n",
    "    # We use strict=False to ignore keys that are in the file but not needed by the model\n",
    "    # (like pixel_mean, pixel_std).\n",
    "    incompatible = model.load_state_dict(corrected_weights, strict=False)\n",
    "    \n",
    "    if not incompatible.missing_keys and not incompatible.unexpected_keys:\n",
    "        print(\"✅ Success! All model weights were loaded correctly after manual correction.\")\n",
    "    else:\n",
    "        print(\"❌ Error! Still some mismatches after correction.\")\n",
    "        print(f\"Missing keys in model: {incompatible.missing_keys}\")\n",
    "        print(f\"Unexpected keys in file: {incompatible.unexpected_keys}\")\n",
    "\n",
    "    print(\"--------------------------------\\n\")\n",
    "\n",
    "    # --- Verification ---\n",
    "    # ... (the rest of the script, like printing the model architecture) ...\n",
    "\n",
    "    # --- Verification ---\n",
    "    # Print the model architecture to verify it has been loaded correctly.\n",
    "    # You should see the ResNet-50 blocks, the pooling layer, and the BNNeck.\n",
    "    print(\"\\n--- Model Architecture ---\")\n",
    "    print(model)\n",
    "    print(\"--------------------------\\n\")\n",
    "    \n",
    "    print(\"FastReID model is ready for feature extraction.\")\n",
    "    \n",
    "    # The 'model' object is now ready to be used for inference.\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loaded_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16dcf40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728760d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reid_model='osnet_x0_25_msmt17.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da92196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model('img1.jpg', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e145850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))  # Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb1fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea57eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vid = cv.VideoCapture(\"Double1.mp4\")\n",
    "# ptime=0\n",
    "# while True:\n",
    "#     isTrue, frame = vid.read()\n",
    "#     if not isTrue:\n",
    "#         print(\"End of video or failed to read frame.\")\n",
    "#         break\n",
    "#     results = model.track(frame, verbose=False)  # device=0 for GPU\n",
    "\n",
    "#     # Draw boxes manually\n",
    "#     boxes = results[0].boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]\n",
    "#     confidences = results[0].boxes.conf.cpu().numpy()\n",
    "#     class_ids = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "#     names = model.names\n",
    "\n",
    "#     for box, conf, cls in zip(boxes, confidences, class_ids):\n",
    "#         x1, y1, x2, y2 = map(int, box)\n",
    "#         label = f\"{names[cls]} {conf:.2f}\"\n",
    "#         cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "#     # FPS calculation\n",
    "#     ctime = time.time()\n",
    "#     fps = 1 / (ctime - ptime)\n",
    "#     ptime = ctime\n",
    "#     cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "#     cv.imshow(\"frame\", frame)\n",
    "#     # ctime=time.time()\n",
    "#     # fps=1/(ctime-ptime)\n",
    "#     # ptime=ctime\n",
    "#     # cv.putText(frame, str(int(fps)), (10, 70), cv.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)\n",
    "#     # cv.imshow(\"frame\",results[0].plot() )\n",
    "#     # cv.imshow(\"frame\", frame)\n",
    "#     # Wait for 20ms, exit if 'd' is pressed\n",
    "#     if cv.waitKey(1) & 0xFF == ord('d'):\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all windows\n",
    "# vid.release()\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70c0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2 as cv\n",
    "# import time\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load YOLOv8n model\n",
    "# # model = YOLO(\"yolov8m.pt\")  # you can also use yolov8m.pt etc.\n",
    "\n",
    "# # Open video\n",
    "# vid = cv.VideoCapture(\"palace.mp4\")\n",
    "# ptime = 0\n",
    "\n",
    "# while True:\n",
    "#     isTrue, frame = vid.read()\n",
    "#     if not isTrue:\n",
    "#         print(\"End of video or failed to read frame.\")\n",
    "#         break\n",
    "\n",
    "#     # Inference (faster if you resize frame before)\n",
    "#     results = model.track(frame, device=0, verbose=False,tracker='bytetrack.yaml',persist=True,classes=[0])  # device=0 for GPU\n",
    "\n",
    "#     # Draw boxes manually\n",
    "#     for r in results:\n",
    "#         if r.boxes.id is not None:\n",
    "#             boxes = r.boxes.xyxy.cpu().numpy()     # [x1, y1, x2, y2]\n",
    "#             ids = r.boxes.id.cpu().numpy().astype(int)\n",
    "#             cls = r.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "#             for box, obj_id, class_id in zip(boxes, ids, cls):\n",
    "#                 x1, y1, x2, y2 = map(int, box)\n",
    "#                 # label = f\"ID {obj_id} - {model.names[class_id]}\"\n",
    "#                 label = f\"ID {obj_id} - {model.names[class_id]}\"\n",
    "#                 cv.rectangle(frame, (x1, y1), (x2, y2), (90, 189,87), 2)\n",
    "#                 cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)\n",
    "#     # FPS calculation\n",
    "#     ctime = time.time()\n",
    "#     fps = 1 / (ctime - ptime)\n",
    "#     ptime = ctime\n",
    "#     cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "#     # Show frame\n",
    "#     cv.imshow(\"YOLOv8 Detection\", frame)\n",
    "\n",
    "#     if cv.waitKey(100) & 0xFF == ord('d'):\n",
    "#         break\n",
    "   \n",
    "\n",
    "# vid.release()\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f36de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptime=0\n",
    "# for result in model2.track(source=0, stream=True, tracker=\"bytetrack.yaml\",persist=True,classes=[0]):\n",
    "#     frame = result.orig_img\n",
    "\n",
    "#     # Draw boxes with IDs\n",
    "#     if result.boxes.id is not None:\n",
    "#         boxes = result.boxes.xyxy.cpu().numpy()\n",
    "#         ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "#         cls = result.boxes.cls.cpu().numpy().astype(int)\n",
    "#         for box, obj_id, class_id in zip(boxes, ids, cls):\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "#             # label = f\"ID {obj_id} - {model.names[class_id]}\"\n",
    "#             if len(boxes) >= 2:\n",
    "#                 label = f\"GAY No. {obj_id}\"\n",
    "#             else :\n",
    "#                 label = f\"Slave No. {obj_id}\"\n",
    "            \n",
    "#             cv.rectangle(frame, (x1, y1), (x2, y2), (90, 189,87), 2)\n",
    "#             cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "#     ctime = time.time()\n",
    "#     fps = 1 / (ctime - ptime)\n",
    "#     ptime = ctime\n",
    "#     cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "#     frame=cv.resize(frame,(1920,1080))\n",
    "#     cv.imshow(\"Tracked\", frame)\n",
    "#     if cv.waitKey(1) == ord('q'):\n",
    "        \n",
    "#         break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058d0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d011eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ptime=0\n",
    "\n",
    "# for result in model3.track(source=\"palace.mp4\", stream=True, tracker=\"bytetrack.yaml\",persist=True,classes=[0],verbose=True):\n",
    "#     frame = result.orig_img\n",
    "#     # tracker_obj = model3.tracker\n",
    "#     # print(\"Tracker class in use:\", tracker_obj.__class__.__name__)\n",
    "#     # with open(\"C:/Users/ayush/anaconda3/envs/fastreid_env/Lib/site-packages/ultralytics/cfg/trackers/botsort.yaml\", \"r\") as f:\n",
    "#     #     print(f.read())\n",
    "\n",
    "#     # Draw boxes with IDs\n",
    "#     if result.boxes.id is not None:\n",
    "#         boxes = result.boxes.xyxy.cpu().numpy()\n",
    "#         ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "#         cls = result.boxes.cls.cpu().numpy().astype(int)\n",
    "#         for box, obj_id, class_id in zip(boxes, ids, cls):\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "#             # label = f\"ID {obj_id} - {model2.names[class_id]}\"\n",
    "#             label = f\"{obj_id}\"\n",
    "            \n",
    "            \n",
    "#             cv.rectangle(frame, (x1, y1), (x2, y2), (90, 189,87), 2)\n",
    "#             cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "#     ctime = time.time()\n",
    "#     fps = 1 / (ctime - ptime)\n",
    "#     ptime = ctime\n",
    "#     cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "#     cv.imshow(\"Tracked\", frame)\n",
    "#     if cv.waitKey(1) == ord('q'):\n",
    "        \n",
    "#         break\n",
    "\n",
    "# cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b54a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastreid in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: faiss-cpu<2.0.0,>=1.7.3 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from fastreid) (1.11.0.post1)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.7.0.68 (from fastreid)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.1 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from fastreid) (1.6.1)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from fastreid) (0.9.0)\n",
      "Requirement already satisfied: tensorboard<3.0.0,>=2.12.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from fastreid) (2.20.0)\n",
      "Requirement already satisfied: termcolor<3.0.0,>=2.2.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from fastreid) (2.5.0)\n",
      "Collecting torch==1.13.1 (from fastreid)\n",
      "  Using cached torch-1.13.1-cp39-cp39-win_amd64.whl.metadata (23 kB)\n",
      "Collecting torchvision==0.14.1 (from fastreid)\n",
      "  Using cached torchvision-0.14.1-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: yacs<0.2.0,>=0.1.8 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from fastreid) (0.1.8)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from torch==1.13.1->fastreid) (4.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from torchvision==0.14.1->fastreid) (2.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from torchvision==0.14.1->fastreid) (2.32.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from torchvision==0.14.1->fastreid) (11.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from faiss-cpu<2.0.0,>=1.7.3->fastreid) (25.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn<2.0.0,>=1.2.1->fastreid) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn<2.0.0,>=1.2.1->fastreid) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from scikit-learn<2.0.0,>=1.2.1->fastreid) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<3.0.0,>=2.12.0->fastreid) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<3.0.0,>=2.12.0->fastreid) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<3.0.0,>=2.12.0->fastreid) (3.8.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<3.0.0,>=2.12.0->fastreid) (3.19.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<3.0.0,>=2.12.0->fastreid) (72.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<3.0.0,>=2.12.0->fastreid) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<3.0.0,>=2.12.0->fastreid) (3.1.3)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from yacs<0.2.0,>=0.1.8->fastreid) (6.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3.0.0,>=2.12.0->fastreid) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3.0.0,>=2.12.0->fastreid) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<3.0.0,>=2.12.0->fastreid) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->torchvision==0.14.1->fastreid) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->torchvision==0.14.1->fastreid) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->torchvision==0.14.1->fastreid) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->torchvision==0.14.1->fastreid) (2025.8.3)\n",
      "Using cached torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "Using cached torchvision-0.14.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "Installing collected packages: torch, opencv-python-headless, torchvision\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.8.0+cu128\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "    Uninstalling torch-2.8.0+cu128:\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "      Successfully uninstalled torch-2.8.0+cu128\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [opencv-python-headless]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ayush\\anaconda3\\envs\\tf\\Lib\\site-packages\\~%rch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\ayush\\\\anaconda3\\\\envs\\\\tf\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install fastreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae166ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fastreid\n",
      "Version: 1.4.0\n",
      "Summary: SOTA Re-identification Methods and Toolbox\n",
      "Home-page: https://github.com/JDAI-CV/fast-reid\n",
      "Author: He, Lingxiao and Liao, Xingyu and Liu, Wu and Liu, Xinchen and Cheng, Peng and Mei, Tao\n",
      "Author-email: \n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages\n",
      "Requires: faiss-cpu, opencv-python-headless, scikit-learn, tabulate, tensorboard, termcolor, torch, torchvision, yacs\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show fastreid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d67219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fastreid\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d79487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.parallel import DistributedDataParallel, DataParallel\n",
    "# import torch.nn as nn\n",
    "\n",
    "# import copy\n",
    "# import logging\n",
    "# from collections.abc import MutableMapping\n",
    "# from typing import Dict, Any\n",
    "# class PathHandler:\n",
    "#     \"\"\"\n",
    "#     PathHandler is a base class that defines common I/O functionality for a URI\n",
    "#     protocol. It routes I/O for a generic URI which may look like \"protocol://*\"\n",
    "#     or a canonical filepath \"/foo/bar/baz\".\n",
    "#     \"\"\"\n",
    "\n",
    "#     _strict_kwargs_check = True\n",
    "# class NativePathHandler(PathHandler):\n",
    "#     \"\"\"\n",
    "#     Handles paths that can be accessed using Python native system calls. This\n",
    "#     handler uses `open()` and `os.*` calls on the given path.\n",
    "#     \"\"\"\n",
    "# class OrderedDict(dict):\n",
    "#     'Dictionary that remembers insertion order'\n",
    "#     # An inherited dict maps keys to values.\n",
    "#     # The inherited dict provides __getitem__, __len__, __contains__, and get.\n",
    "#     # The remaining methods are order-aware.\n",
    "#     # Big-O running times for all methods are the same as regular dictionaries.\n",
    "\n",
    "#     # The internal self.__map dict maps keys to links in a doubly linked list.\n",
    "#     # The circular doubly linked list starts and ends with a sentinel element.\n",
    "#     # The sentinel element never gets deleted (this simplifies the algorithm).\n",
    "#     # The sentinel is in self.__hardroot with a weakref proxy in self.__root.\n",
    "#     # The prev links are weakref proxies (to prevent circular references).\n",
    "#     # Individual links are kept alive by the hard reference in self.__map.\n",
    "#     # Those hard references disappear when a key is deleted from an OrderedDict.\n",
    "\n",
    "# class PathHandler:\n",
    "#     \"\"\"\n",
    "#     PathHandler is a base class that defines common I/O functionality for a URI\n",
    "#     protocol. It routes I/O for a generic URI which may look like \"protocol://*\"\n",
    "#     or a canonical filepath \"/foo/bar/baz\".\n",
    "#     \"\"\"\n",
    "\n",
    "#     _strict_kwargs_check = True\n",
    "\n",
    "#     def _check_kwargs(self, kwargs: Dict[str, Any]) -> None:\n",
    "#         \"\"\"\n",
    "#         Checks if the given arguments are empty. Throws a ValueError if strict\n",
    "#         kwargs checking is enabled and args are non-empty. If strict kwargs\n",
    "#         checking is disabled, only a warning is logged.\n",
    "#         Args:\n",
    "#             kwargs (Dict[str, Any])\n",
    "#         \"\"\"\n",
    "#         if self._strict_kwargs_check:\n",
    "#             if len(kwargs) > 0:\n",
    "#                 raise ValueError(\"Unused arguments: {}\".format(kwargs))\n",
    "#         else:\n",
    "#             logger = logging.getLogger(__name__)\n",
    "#             for k, v in kwargs.items():\n",
    "#                 logger.warning(\n",
    "#                     \"[PathManager] {}={} argument ignored\".format(k, v)\n",
    "#                 )\n",
    "# class PathManager:\n",
    "#     \"\"\"\n",
    "#     A class for users to open generic paths or translate generic paths to file names.\n",
    "#     \"\"\"\n",
    "\n",
    "#     _PATH_HANDLERS: MutableMapping[str, PathHandler] = OrderedDict()\n",
    "#     _NATIVE_PATH_HANDLER = NativePathHandler()\n",
    "\n",
    "#     @staticmethod\n",
    "#     def __get_path_handler(path: str) -> PathHandler:\n",
    "#         \"\"\"\n",
    "#         Finds a PathHandler that supports the given path. Falls back to the native\n",
    "#         PathHandler if no other handler is found.\n",
    "#         Args:\n",
    "#             path (str): URI path to resource\n",
    "#         Returns:\n",
    "#             handler (PathHandler)\n",
    "#         \"\"\"\n",
    "#         for p in PathManager._PATH_HANDLERS.keys():\n",
    "#             if path.startswith(p):\n",
    "#                 return PathManager._PATH_HANDLERS[p]\n",
    "#         return PathManager._NATIVE_PATH_HANDLER\n",
    "\n",
    "# class Checkpointer(object):\n",
    "#     \"\"\"\n",
    "#     A checkpointer that can save/load model as well as extra checkpointable\n",
    "#     objects.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             model: nn.Module,\n",
    "#             save_dir: str = \"\",\n",
    "#             *,\n",
    "#             save_to_disk: bool = True,\n",
    "#             **checkpointables: object,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             model (nn.Module): model.\n",
    "#             save_dir (str): a directory to save and find checkpoints.\n",
    "#             save_to_disk (bool): if True, save checkpoint to disk, otherwise\n",
    "#                 disable saving for this checkpointer.\n",
    "#             checkpointables (object): any checkpointable objects, i.e., objects\n",
    "#                 that have the `state_dict()` and `load_state_dict()` method. For\n",
    "#                 example, it can be used like\n",
    "#                 `Checkpointer(model, \"dir\", optimizer=optimizer)`.\n",
    "#         \"\"\"\n",
    "#         if isinstance(model, (DistributedDataParallel, DataParallel)):\n",
    "#             model = model.module\n",
    "#         self.model = model\n",
    "#         self.checkpointables = copy.copy(checkpointables)\n",
    "#         self.logger = logging.getLogger(__name__)\n",
    "#         self.save_dir = save_dir\n",
    "#         self.save_to_disk = save_to_disk\n",
    "\n",
    "#         self.path_manager = PathManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "732d0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _type_check(arg, msg, is_argument=True, module=None, *, allow_special_forms=False):\n",
    "#     \"\"\"Check that the argument is a type, and return it (internal helper).\n",
    "\n",
    "#     As a special case, accept None and return type(None) instead. Also wrap strings\n",
    "#     into ForwardRef instances. Consider several corner cases, for example plain\n",
    "#     special forms like Union are not valid, while Union[int, str] is OK, etc.\n",
    "#     The msg argument is a human-readable error message, e.g.::\n",
    "\n",
    "#         \"Union[arg, ...]: arg should be a type.\"\n",
    "\n",
    "#     We append the repr() of the actual value (truncated to 100 chars).\n",
    "#     \"\"\"\n",
    "#     invalid_generic_forms = (Generic, Protocol)\n",
    "#     if not allow_special_forms:\n",
    "#         invalid_generic_forms += (ClassVar,)\n",
    "#         if is_argument:\n",
    "#             invalid_generic_forms += (Final,)\n",
    "\n",
    "#     arg = _type_convert(arg, module=module, allow_special_forms=allow_special_forms)\n",
    "#     if (isinstance(arg, _GenericAlias) and\n",
    "#             arg.__origin__ in invalid_generic_forms):\n",
    "#         raise TypeError(f\"{arg} is not valid as type argument\")\n",
    "#     if arg in (Any, LiteralString, NoReturn, Never, Self, TypeAlias):\n",
    "#         return arg\n",
    "#     if allow_special_forms and arg in (ClassVar, Final):\n",
    "#         return arg\n",
    "#     if isinstance(arg, _SpecialForm) or arg in (Generic, Protocol):\n",
    "#         raise TypeError(f\"Plain {arg} is not valid as type argument\")\n",
    "#     if type(arg) is tuple:\n",
    "#         raise TypeError(f\"{msg} Got {arg!r:.100}.\")\n",
    "#     return arg\n",
    "# def Optional(self, parameters):\n",
    "#     \"\"\"Optional[X] is equivalent to Union[X, None].\"\"\"\n",
    "#     arg = _type_check(parameters, f\"{self} requires a single type.\")\n",
    "#     return Union[arg, type(None)]\n",
    "\n",
    "# def load(self, path: str, checkpointables: Optional[List[str]] = None) -> object:\n",
    "#         \"\"\"\n",
    "#         Load from the given checkpoint. When path points to network file, this\n",
    "#         function has to be called on all ranks.\n",
    "\n",
    "#         Args:\n",
    "#             path (str): path or url to the checkpoint. If empty, will not load\n",
    "#                 anything.\n",
    "#             checkpointables (list): List of checkpointable names to load. If not\n",
    "#                 specified (None), will load all the possible checkpointables.\n",
    "#         Returns:\n",
    "#             dict:\n",
    "#                 extra data loaded from the checkpoint that has not been\n",
    "#                 processed. For example, those saved with\n",
    "#                 :meth:`.save(**extra_data)`.\n",
    "#         \"\"\"\n",
    "#         if not path:\n",
    "#             # no checkpoint provided\n",
    "#             self.logger.info(\"No checkpoint found. Training model from scratch\")\n",
    "#             return {}\n",
    "#         self.logger.info(\"Loading checkpoint from {}\".format(path))\n",
    "#         if not os.path.isfile(path):\n",
    "#             path = self.path_manager.get_local_path(path)\n",
    "#             assert os.path.isfile(path), \"Checkpoint {} not found!\".format(path)\n",
    "\n",
    "#         checkpoint = self._load_file(path)\n",
    "#         incompatible = self._load_model(checkpoint)\n",
    "#         if (\n",
    "#                 incompatible is not None\n",
    "#         ):  # handle some existing subclasses that returns None\n",
    "#             self._log_incompatible_keys(incompatible)\n",
    "\n",
    "#         for key in self.checkpointables if checkpointables is None else checkpointables:\n",
    "#             if key in checkpoint:  # pyre-ignore\n",
    "#                 self.logger.info(\"Loading {} from {}\".format(key, path))\n",
    "#                 obj = self.checkpointables[key]\n",
    "#                 obj.load_state_dict(checkpoint.pop(key))  # pyre-ignore\n",
    "\n",
    "#         # return any further checkpoint data\n",
    "#         return checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0de6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastreid.modeling import build_model\n",
    "\n",
    "\n",
    "# class DefaultPredictor:\n",
    "#     \"\"\"\n",
    "#     Create a simple end-to-end predictor with the given config.\n",
    "#     The predictor takes an BGR image, resizes it to the specified resolution,\n",
    "#     runs the model and produces a dict of predictions.\n",
    "#     This predictor takes care of model loading and input preprocessing for you.\n",
    "#     If you'd like to do anything more fancy, please refer to its source code\n",
    "#     as examples to build and use the model manually.\n",
    "#     Attributes:\n",
    "#     Examples:\n",
    "#     .. code-block:: python\n",
    "#         pred = DefaultPredictor(cfg)\n",
    "#         inputs = cv2.imread(\"input.jpg\")\n",
    "#         outputs = pred(inputs)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, cfg):\n",
    "#         self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "#         self.cfg.defrost()\n",
    "#         self.cfg.MODEL.BACKBONE.PRETRAIN = False\n",
    "#         self.model = build_model(self.cfg)\n",
    "#         self.model.eval()\n",
    "\n",
    "#         Checkpointer(self.model).load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "#     def __call__(self, image):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             image (torch.tensor): an image tensor of shape (B, C, H, W).\n",
    "#         Returns:\n",
    "#             predictions (torch.tensor): the output features of the model\n",
    "#         \"\"\"\n",
    "#         inputs = {\"images\": image.to(self.model.device)}\n",
    "#         with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "#             predictions = self.model(inputs)\n",
    "#         return predictions.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a788321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"C:/CCTV/fast-reid/fastreid/engine\")\n",
    "# import defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6bb85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "from typing import Optional, List, Dict, NamedTuple, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from termcolor import colored\n",
    "from torch.nn.parallel import DistributedDataParallel, DataParallel\n",
    "\n",
    "from fastreid.utils.file_io import PathManager\n",
    "\n",
    "\n",
    "class _IncompatibleKeys(\n",
    "    NamedTuple(\n",
    "        # pyre-fixme[10]: Name `IncompatibleKeys` is used but not defined.\n",
    "        \"IncompatibleKeys\",\n",
    "        [\n",
    "            (\"missing_keys\", List[str]),\n",
    "            (\"unexpected_keys\", List[str]),\n",
    "            # pyre-fixme[24]: Generic type `tuple` expects at least 1 type parameter.\n",
    "            # pyre-fixme[24]: Generic type `tuple` expects at least 1 type parameter.\n",
    "            # pyre-fixme[24]: Generic type `tuple` expects at least 1 type parameter.\n",
    "            (\"incorrect_shapes\", List[Tuple]),\n",
    "        ],\n",
    "    )\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Checkpointer(object):\n",
    "    \"\"\"\n",
    "    A checkpointer that can save/load model as well as extra checkpointable\n",
    "    objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            save_dir: str = \"\",\n",
    "            *,\n",
    "            save_to_disk: bool = True,\n",
    "            **checkpointables: object,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): model.\n",
    "            save_dir (str): a directory to save and find checkpoints.\n",
    "            save_to_disk (bool): if True, save checkpoint to disk, otherwise\n",
    "                disable saving for this checkpointer.\n",
    "            checkpointables (object): any checkpointable objects, i.e., objects\n",
    "                that have the `state_dict()` and `load_state_dict()` method. For\n",
    "                example, it can be used like\n",
    "                `Checkpointer(model, \"dir\", optimizer=optimizer)`.\n",
    "        \"\"\"\n",
    "        if isinstance(model, (DistributedDataParallel, DataParallel)):\n",
    "            model = model.module\n",
    "        self.model = model\n",
    "        self.checkpointables = copy.copy(checkpointables)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.save_dir = save_dir\n",
    "        self.save_to_disk = save_to_disk\n",
    "\n",
    "        self.path_manager = PathManager\n",
    "\n",
    "    def save(self, name: str, **kwargs: Dict[str, str]):\n",
    "        \"\"\"\n",
    "        Dump model and checkpointables to a file.\n",
    "\n",
    "        Args:\n",
    "            name (str): name of the file.\n",
    "            kwargs (dict): extra arbitrary data to save.\n",
    "        \"\"\"\n",
    "        if not self.save_dir or not self.save_to_disk:\n",
    "            return\n",
    "\n",
    "        data = {}\n",
    "        data[\"model\"] = self.model.state_dict()\n",
    "        for key, obj in self.checkpointables.items():\n",
    "            data[key] = obj.state_dict()\n",
    "        data.update(kwargs)\n",
    "\n",
    "        basename = \"{}.pth\".format(name)\n",
    "        save_file = os.path.join(self.save_dir, basename)\n",
    "        assert os.path.basename(save_file) == basename, basename\n",
    "        self.logger.info(\"Saving checkpoint to {}\".format(save_file))\n",
    "        with PathManager.open(save_file, \"wb\") as f:\n",
    "            torch.save(data, f)\n",
    "        self.tag_last_checkpoint(basename)\n",
    "\n",
    "    def load(self, path: str, checkpointables: Optional[List[str]] = None) -> object:\n",
    "        \"\"\"\n",
    "        Load from the given checkpoint. When path points to network file, this\n",
    "        function has to be called on all ranks.\n",
    "\n",
    "        Args:\n",
    "            path (str): path or url to the checkpoint. If empty, will not load\n",
    "                anything.\n",
    "            checkpointables (list): List of checkpointable names to load. If not\n",
    "                specified (None), will load all the possible checkpointables.\n",
    "        Returns:\n",
    "            dict:\n",
    "                extra data loaded from the checkpoint that has not been\n",
    "                processed. For example, those saved with\n",
    "                :meth:`.save(**extra_data)`.\n",
    "        \"\"\"\n",
    "        if not path:\n",
    "            # no checkpoint provided\n",
    "            self.logger.info(\"No checkpoint found. Training model from scratch\")\n",
    "            return {}\n",
    "        self.logger.info(\"Loading checkpoint from {}\".format(path))\n",
    "        if not os.path.isfile(path):\n",
    "            path = self.path_manager.get_local_path(path)\n",
    "            assert os.path.isfile(path), \"Checkpoint {} not found!\".format(path)\n",
    "\n",
    "        checkpoint = self._load_file(path)\n",
    "        incompatible = self._load_model(checkpoint)\n",
    "        if (\n",
    "                incompatible is not None\n",
    "        ):  # handle some existing subclasses that returns None\n",
    "            self._log_incompatible_keys(incompatible)\n",
    "\n",
    "        for key in self.checkpointables if checkpointables is None else checkpointables:\n",
    "            if key in checkpoint:  # pyre-ignore\n",
    "                self.logger.info(\"Loading {} from {}\".format(key, path))\n",
    "                obj = self.checkpointables[key]\n",
    "                obj.load_state_dict(checkpoint.pop(key))  # pyre-ignore\n",
    "\n",
    "        # return any further checkpoint data\n",
    "        return checkpoint\n",
    "\n",
    "    def has_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            bool: whether a checkpoint exists in the target directory.\n",
    "        \"\"\"\n",
    "        save_file = os.path.join(self.save_dir, \"last_checkpoint\")\n",
    "        return PathManager.exists(save_file)\n",
    "\n",
    "    def get_checkpoint_file(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            str: The latest checkpoint file in target directory.\n",
    "        \"\"\"\n",
    "        save_file = os.path.join(self.save_dir, \"last_checkpoint\")\n",
    "        try:\n",
    "            with PathManager.open(save_file, \"r\") as f:\n",
    "                last_saved = f.read().strip()\n",
    "        except IOError:\n",
    "            # if file doesn't exist, maybe because it has just been\n",
    "            # deleted by a separate process\n",
    "            return \"\"\n",
    "        return os.path.join(self.save_dir, last_saved)\n",
    "\n",
    "    def get_all_checkpoint_files(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list: All available checkpoint files (.pth files) in target\n",
    "                directory.\n",
    "        \"\"\"\n",
    "        all_model_checkpoints = [\n",
    "            os.path.join(self.save_dir, file)\n",
    "            for file in PathManager.ls(self.save_dir)\n",
    "            if PathManager.isfile(os.path.join(self.save_dir, file))\n",
    "               and file.endswith(\".pth\")\n",
    "        ]\n",
    "        return all_model_checkpoints\n",
    "\n",
    "    def resume_or_load(self, path: str, *, resume: bool = True):\n",
    "        \"\"\"\n",
    "        If `resume` is True, this method attempts to resume from the last\n",
    "        checkpoint, if exists. Otherwise, load checkpoint from the given path.\n",
    "        This is useful when restarting an interrupted training job.\n",
    "\n",
    "        Args:\n",
    "            path (str): path to the checkpoint.\n",
    "            resume (bool): if True, resume from the last checkpoint if it exists.\n",
    "        Returns:\n",
    "            same as :meth:`load`.\n",
    "        \"\"\"\n",
    "        if resume and self.has_checkpoint():\n",
    "            path = self.get_checkpoint_file()\n",
    "            return self.load(path)\n",
    "        else:\n",
    "            return self.load(path, checkpointables=[])\n",
    "\n",
    "    def tag_last_checkpoint(self, last_filename_basename: str):\n",
    "        \"\"\"\n",
    "        Tag the last checkpoint.\n",
    "\n",
    "        Args:\n",
    "            last_filename_basename (str): the basename of the last filename.\n",
    "        \"\"\"\n",
    "        save_file = os.path.join(self.save_dir, \"last_checkpoint\")\n",
    "        with PathManager.open(save_file, \"w\") as f:\n",
    "            f.write(last_filename_basename)\n",
    "\n",
    "    def _load_file(self, f: str):\n",
    "        \"\"\"\n",
    "        Load a checkpoint file. Can be overwritten by subclasses to support\n",
    "        different formats.\n",
    "\n",
    "        Args:\n",
    "            f (str): a locally mounted file path.\n",
    "        Returns:\n",
    "            dict: with keys \"model\" and optionally others that are saved by\n",
    "                the checkpointer dict[\"model\"] must be a dict which maps strings\n",
    "                to torch.Tensor or numpy arrays.\n",
    "        \"\"\"\n",
    "        return torch.load(f, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    def _load_model(self, checkpoint: Any):\n",
    "        \"\"\"\n",
    "        Load weights from a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint (Any): checkpoint contains the weights.\n",
    "        \"\"\"\n",
    "        checkpoint_state_dict = checkpoint.pop(\"model\")\n",
    "        self._convert_ndarray_to_tensor(checkpoint_state_dict)\n",
    "\n",
    "        # if the state_dict comes from a model that was wrapped in a\n",
    "        # DataParallel or DistributedDataParallel during serialization,\n",
    "        # remove the \"module\" prefix before performing the matching.\n",
    "        _strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n",
    "\n",
    "        # work around https://github.com/pytorch/pytorch/issues/24139\n",
    "        model_state_dict = self.model.state_dict()\n",
    "        incorrect_shapes = []\n",
    "        for k in list(checkpoint_state_dict.keys()):\n",
    "            if k in model_state_dict:\n",
    "                shape_model = tuple(model_state_dict[k].shape)\n",
    "                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n",
    "                if shape_model != shape_checkpoint:\n",
    "                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n",
    "                    checkpoint_state_dict.pop(k)\n",
    "\n",
    "        incompatible = self.model.load_state_dict(checkpoint_state_dict, strict=False)\n",
    "        return _IncompatibleKeys(\n",
    "            missing_keys=incompatible.missing_keys,\n",
    "            unexpected_keys=incompatible.unexpected_keys,\n",
    "            incorrect_shapes=incorrect_shapes,\n",
    "        )\n",
    "\n",
    "    def _log_incompatible_keys(self, incompatible: _IncompatibleKeys) -> None:\n",
    "        \"\"\"\n",
    "        Log information about the incompatible keys returned by ``_load_model``.\n",
    "        \"\"\"\n",
    "        for k, shape_checkpoint, shape_model in incompatible.incorrect_shapes:\n",
    "            self.logger.warning(\n",
    "                \"Skip loading parameter '{}' to the model due to incompatible \"\n",
    "                \"shapes: {} in the checkpoint but {} in the \"\n",
    "                \"model! You might want to double check if this is expected.\".format(\n",
    "                    k, shape_checkpoint, shape_model\n",
    "                )\n",
    "            )\n",
    "        if incompatible.missing_keys:\n",
    "            missing_keys = _filter_reused_missing_keys(\n",
    "                self.model, incompatible.missing_keys\n",
    "            )\n",
    "            if missing_keys:\n",
    "                self.logger.info(get_missing_parameters_message(missing_keys))\n",
    "        if incompatible.unexpected_keys:\n",
    "            self.logger.info(\n",
    "                get_unexpected_parameters_message(incompatible.unexpected_keys)\n",
    "            )\n",
    "\n",
    "    def _convert_ndarray_to_tensor(self, state_dict: dict):\n",
    "        \"\"\"\n",
    "        In-place convert all numpy arrays in the state_dict to torch tensor.\n",
    "\n",
    "        Args:\n",
    "            state_dict (dict): a state-dict to be loaded to the model.\n",
    "        \"\"\"\n",
    "        # model could be an OrderedDict with _metadata attribute\n",
    "        # (as returned by Pytorch's state_dict()). We should preserve these\n",
    "        # properties.\n",
    "        for k in list(state_dict.keys()):\n",
    "            v = state_dict[k]\n",
    "            if not isinstance(v, np.ndarray) and not isinstance(\n",
    "                    v, torch.Tensor\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Unsupported type found in checkpoint! {}: {}\".format(\n",
    "                        k, type(v)\n",
    "                    )\n",
    "                )\n",
    "            if not isinstance(v, torch.Tensor):\n",
    "                state_dict[k] = torch.from_numpy(v)\n",
    "\n",
    "\n",
    "class PeriodicCheckpointer:\n",
    "    \"\"\"\n",
    "    Save checkpoints periodically. When `.step(iteration)` is called, it will\n",
    "    execute `checkpointer.save` on the given checkpointer, if iteration is a\n",
    "    multiple of period or if `max_iter` is reached.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, checkpointer: Any, period: int, max_epoch: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            checkpointer (Any): the checkpointer object used to save\n",
    "            checkpoints.\n",
    "            period (int): the period to save checkpoint.\n",
    "            max_epoch (int): maximum number of epochs. When it is reached,\n",
    "                a checkpoint named \"model_final\" will be saved.\n",
    "        \"\"\"\n",
    "        self.checkpointer = checkpointer\n",
    "        self.period = int(period)\n",
    "        self.max_epoch = max_epoch\n",
    "        self.best_metric = -1\n",
    "\n",
    "    def step(self, epoch: int, **kwargs: Any):\n",
    "        \"\"\"\n",
    "        Perform the appropriate action at the given iteration.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): the current epoch, ranged in [0, max_epoch-1].\n",
    "            kwargs (Any): extra data to save, same as in\n",
    "                :meth:`Checkpointer.save`.\n",
    "        \"\"\"\n",
    "        epoch = int(epoch)\n",
    "        additional_state = {\"epoch\": epoch}\n",
    "        additional_state.update(kwargs)\n",
    "        if (epoch + 1) % self.period == 0 and epoch < self.max_epoch - 1:\n",
    "            if additional_state[\"metric\"] > self.best_metric:\n",
    "                self.checkpointer.save(\n",
    "                    \"model_best\", **additional_state\n",
    "                )\n",
    "                self.best_metric = additional_state[\"metric\"]\n",
    "            # Put it behind best model save to make last checkpoint valid\n",
    "            self.checkpointer.save(\n",
    "                \"model_{:04d}\".format(epoch), **additional_state\n",
    "            )\n",
    "        if epoch >= self.max_epoch - 1:\n",
    "            if additional_state[\"metric\"] > self.best_metric:\n",
    "                self.checkpointer.save(\n",
    "                    \"model_best\", **additional_state\n",
    "                )\n",
    "            self.checkpointer.save(\"model_final\", **additional_state)\n",
    "\n",
    "    def save(self, name: str, **kwargs: Any):\n",
    "        \"\"\"\n",
    "        Same argument as :meth:`Checkpointer.save`.\n",
    "        Use this method to manually save checkpoints outside the schedule.\n",
    "\n",
    "        Args:\n",
    "            name (str): file name.\n",
    "            kwargs (Any): extra data to save, same as in\n",
    "                :meth:`Checkpointer.save`.\n",
    "        \"\"\"\n",
    "        self.checkpointer.save(name, **kwargs)\n",
    "\n",
    "\n",
    "def _filter_reused_missing_keys(model: nn.Module, keys: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter \"missing keys\" to not include keys that have been loaded with another name.\n",
    "    \"\"\"\n",
    "    keyset = set(keys)\n",
    "    param_to_names = defaultdict(set)  # param -> names that points to it\n",
    "    for module_prefix, module in _named_modules_with_dup(model):\n",
    "        for name, param in list(module.named_parameters(recurse=False)) + list(\n",
    "                module.named_buffers(recurse=False)  # pyre-ignore\n",
    "        ):\n",
    "            full_name = (module_prefix + \".\" if module_prefix else \"\") + name\n",
    "            param_to_names[param].add(full_name)\n",
    "    for names in param_to_names.values():\n",
    "        # if one name appears missing but its alias exists, then this\n",
    "        # name is not considered missing\n",
    "        if any(n in keyset for n in names) and not all(n in keyset for n in names):\n",
    "            [keyset.remove(n) for n in names if n in keyset]\n",
    "    return list(keyset)\n",
    "\n",
    "\n",
    "def get_missing_parameters_message(keys: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Get a logging-friendly message to report parameter names (keys) that are in\n",
    "    the model but not found in a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        keys (list[str]): List of keys that were not found in the checkpoint.\n",
    "    Returns:\n",
    "        str: message.\n",
    "    \"\"\"\n",
    "    groups = _group_checkpoint_keys(keys)\n",
    "    msg = \"Some model parameters or buffers are not found in the checkpoint:\\n\"\n",
    "    msg += \"\\n\".join(\n",
    "        \"  \" + colored(k + _group_to_str(v), \"blue\") for k, v in groups.items()\n",
    "    )\n",
    "    return msg\n",
    "\n",
    "\n",
    "def get_unexpected_parameters_message(keys: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Get a logging-friendly message to report parameter names (keys) that are in\n",
    "    the checkpoint but not found in the model.\n",
    "\n",
    "    Args:\n",
    "        keys (list[str]): List of keys that were not found in the model.\n",
    "    Returns:\n",
    "        str: message.\n",
    "    \"\"\"\n",
    "    groups = _group_checkpoint_keys(keys)\n",
    "    msg = \"The checkpoint state_dict contains keys that are not used by the model:\\n\"\n",
    "    msg += \"\\n\".join(\n",
    "        \"  \" + colored(k + _group_to_str(v), \"magenta\") for k, v in groups.items()\n",
    "    )\n",
    "    return msg\n",
    "\n",
    "\n",
    "def _strip_prefix_if_present(state_dict: Dict[str, Any], prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Strip the prefix in metadata, if any.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): a state-dict to be loaded to the model.\n",
    "        prefix (str): prefix.\n",
    "    \"\"\"\n",
    "    keys = sorted(state_dict.keys())\n",
    "    if not all(len(key) == 0 or key.startswith(prefix) for key in keys):\n",
    "        return\n",
    "\n",
    "    for key in keys:\n",
    "        newkey = key[len(prefix):]\n",
    "        state_dict[newkey] = state_dict.pop(key)\n",
    "\n",
    "    # also strip the prefix in metadata, if any..\n",
    "    try:\n",
    "        metadata = state_dict._metadata  # pyre-ignore\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        for key in list(metadata.keys()):\n",
    "            # for the metadata dict, the key can be:\n",
    "            # '': for the DDP module, which we want to remove.\n",
    "            # 'module': for the actual model.\n",
    "            # 'module.xx.xx': for the rest.\n",
    "\n",
    "            if len(key) == 0:\n",
    "                continue\n",
    "            newkey = key[len(prefix):]\n",
    "            metadata[newkey] = metadata.pop(key)\n",
    "\n",
    "\n",
    "def _group_checkpoint_keys(keys: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Group keys based on common prefixes. A prefix is the string up to the final\n",
    "    \".\" in each key.\n",
    "\n",
    "    Args:\n",
    "        keys (list[str]): list of parameter names, i.e. keys in the model\n",
    "            checkpoint dict.\n",
    "    Returns:\n",
    "        dict[list]: keys with common prefixes are grouped into lists.\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for key in keys:\n",
    "        pos = key.rfind(\".\")\n",
    "        if pos >= 0:\n",
    "            head, tail = key[:pos], [key[pos + 1:]]\n",
    "        else:\n",
    "            head, tail = key, []\n",
    "        groups[head].extend(tail)\n",
    "    return groups\n",
    "\n",
    "\n",
    "def _group_to_str(group: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Format a group of parameter name suffixes into a loggable string.\n",
    "\n",
    "    Args:\n",
    "        group (list[str]): list of parameter name suffixes.\n",
    "    Returns:\n",
    "        str: formated string.\n",
    "    \"\"\"\n",
    "    if len(group) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    if len(group) == 1:\n",
    "        return \".\" + group[0]\n",
    "\n",
    "    return \".{\" + \", \".join(group) + \"}\"\n",
    "\n",
    "\n",
    "def _named_modules_with_dup(\n",
    "        model: nn.Module, prefix: str = \"\"\n",
    ") -> Iterable[Tuple[str, nn.Module]]:\n",
    "    \"\"\"\n",
    "    The same as `model.named_modules()`, except that it includes\n",
    "    duplicated modules that have more than one name.\n",
    "    \"\"\"\n",
    "    yield prefix, model\n",
    "    for name, module in model._modules.items():  # pyre-ignore\n",
    "        if module is None:\n",
    "            continue\n",
    "        submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n",
    "        yield from _named_modules_with_dup(module, submodule_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e72fed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "#train-loop\n",
    "\"\"\"\n",
    "credit:\n",
    "https://github.com/facebookresearch/detectron2/blob/master/detectron2/engine/train_loop.py\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import time\n",
    "import weakref\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DataParallel, DistributedDataParallel\n",
    "\n",
    "import fastreid.utils.comm as comm\n",
    "from fastreid.utils.events import EventStorage, get_event_storage\n",
    "from fastreid.utils.params import ContiguousParams\n",
    "\n",
    "__all__ = [\"HookBase\", \"TrainerBase\", \"SimpleTrainer\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class HookBase:\n",
    "    \"\"\"\n",
    "    Base class for hooks that can be registered with :class:`TrainerBase`.\n",
    "    Each hook can implement 6 methods. The way they are called is demonstrated\n",
    "    in the following snippet:\n",
    "    .. code-block:: python\n",
    "        hook.before_train()\n",
    "        for _ in range(start_epoch, max_epoch):\n",
    "            hook.before_epoch()\n",
    "            for iter in range(start_iter, max_iter):\n",
    "                hook.before_step()\n",
    "                trainer.run_step()\n",
    "                hook.after_step()\n",
    "            hook.after_epoch()\n",
    "        hook.after_train()\n",
    "    Notes:\n",
    "        1. In the hook method, users can access `self.trainer` to access more\n",
    "           properties about the context (e.g., current iteration).\n",
    "        2. A hook that does something in :meth:`before_step` can often be\n",
    "           implemented equivalently in :meth:`after_step`.\n",
    "           If the hook takes non-trivial time, it is strongly recommended to\n",
    "           implement the hook in :meth:`after_step` instead of :meth:`before_step`.\n",
    "           The convention is that :meth:`before_step` should only take negligible time.\n",
    "           Following this convention will allow hooks that do care about the difference\n",
    "           between :meth:`before_step` and :meth:`after_step` (e.g., timer) to\n",
    "           function properly.\n",
    "    Attributes:\n",
    "        trainer: A weak reference to the trainer object. Set by the trainer when the hook is\n",
    "            registered.\n",
    "    \"\"\"\n",
    "\n",
    "    def before_train(self):\n",
    "        \"\"\"\n",
    "        Called before the first iteration.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_train(self):\n",
    "        \"\"\"\n",
    "        Called after the last iteration.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_epoch(self):\n",
    "        \"\"\"\n",
    "        Called before each epoch.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"\"\"\n",
    "        Called after each epoch.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_step(self):\n",
    "        \"\"\"\n",
    "        Called before each iteration.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_step(self):\n",
    "        \"\"\"\n",
    "        Called after each iteration.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TrainerBase:\n",
    "    \"\"\"\n",
    "    Base class for iterative trainer with hooks.\n",
    "    The only assumption we made here is: the training runs in a loop.\n",
    "    A subclass can implement what the loop is.\n",
    "    We made no assumptions about the existence of dataloader, optimizer, model, etc.\n",
    "    Attributes:\n",
    "        iter(int): the current iteration.\n",
    "        epoch(int): the current epoch.\n",
    "        start_iter(int): The iteration to start with.\n",
    "            By convention the minimum possible value is 0.\n",
    "        max_epoch (int): The epoch to end training.\n",
    "        storage(EventStorage): An EventStorage that's opened during the course of training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._hooks = []\n",
    "\n",
    "    def register_hooks(self, hooks):\n",
    "        \"\"\"\n",
    "        Register hooks to the trainer. The hooks are executed in the order\n",
    "        they are registered.\n",
    "        Args:\n",
    "            hooks (list[Optional[HookBase]]): list of hooks\n",
    "        \"\"\"\n",
    "        hooks = [h for h in hooks if h is not None]\n",
    "        for h in hooks:\n",
    "            assert isinstance(h, HookBase)\n",
    "            # To avoid circular reference, hooks and trainer cannot own each other.\n",
    "            # This normally does not matter, but will cause memory leak if the\n",
    "            # involved objects contain __del__:\n",
    "            # See http://engineering.hearsaysocial.com/2013/06/16/circular-references-in-python/\n",
    "            h.trainer = weakref.proxy(self)\n",
    "        self._hooks.extend(hooks)\n",
    "\n",
    "    def train(self, start_epoch: int, max_epoch: int, iters_per_epoch: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            start_epoch, max_epoch (int): See docs above\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Starting training from epoch {}\".format(start_epoch))\n",
    "\n",
    "        self.iter = self.start_iter = start_epoch * iters_per_epoch\n",
    "\n",
    "        with EventStorage(self.start_iter) as self.storage:\n",
    "            try:\n",
    "                self.before_train()\n",
    "                for self.epoch in range(start_epoch, max_epoch):\n",
    "                    self.before_epoch()\n",
    "                    for _ in range(iters_per_epoch):\n",
    "                        self.before_step()\n",
    "                        self.run_step()\n",
    "                        self.after_step()\n",
    "                        self.iter += 1\n",
    "                    self.after_epoch()\n",
    "            except Exception:\n",
    "                logger.exception(\"Exception during training:\")\n",
    "                raise\n",
    "            finally:\n",
    "                self.after_train()\n",
    "\n",
    "    def before_train(self):\n",
    "        for h in self._hooks:\n",
    "            h.before_train()\n",
    "\n",
    "    def after_train(self):\n",
    "        self.storage.iter = self.iter\n",
    "        for h in self._hooks:\n",
    "            h.after_train()\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.storage.epoch = self.epoch\n",
    "\n",
    "        for h in self._hooks:\n",
    "            h.before_epoch()\n",
    "\n",
    "    def before_step(self):\n",
    "        self.storage.iter = self.iter\n",
    "\n",
    "        for h in self._hooks:\n",
    "            h.before_step()\n",
    "\n",
    "    def after_step(self):\n",
    "        for h in self._hooks:\n",
    "            h.after_step()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        for h in self._hooks:\n",
    "            h.after_epoch()\n",
    "\n",
    "    def run_step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SimpleTrainer(TrainerBase):\n",
    "    \"\"\"\n",
    "    A simple trainer for the most common type of task:\n",
    "    single-cost single-optimizer single-data-source iterative optimization.\n",
    "    It assumes that every step, you:\n",
    "    1. Compute the loss with a data from the data_loader.\n",
    "    2. Compute the gradients with the above loss.\n",
    "    3. Update the model with the optimizer.\n",
    "    If you want to do anything fancier than this,\n",
    "    either subclass TrainerBase and implement your own `run_step`,\n",
    "    or write your own training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data_loader, optimizer, param_wrapper):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: a torch Module. Takes a data from data_loader and returns a\n",
    "                dict of heads.\n",
    "            data_loader: an iterable. Contains data to be used to call model.\n",
    "            optimizer: a torch optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        We set the model to training mode in the trainer.\n",
    "        However it's valid to train a model that's in eval mode.\n",
    "        If you want your model (or a submodule of it) to behave\n",
    "        like evaluation during training, you can overwrite its train() method.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self._data_loader_iter = iter(data_loader)\n",
    "        self.optimizer = optimizer\n",
    "        self.param_wrapper = param_wrapper\n",
    "\n",
    "    def run_step(self):\n",
    "        \"\"\"\n",
    "        Implement the standard training logic described above.\n",
    "        \"\"\"\n",
    "        assert self.model.training, \"[SimpleTrainer] model was changed to eval mode!\"\n",
    "        start = time.perf_counter()\n",
    "        \"\"\"\n",
    "        If your want to do something with the data, you can wrap the dataloader.\n",
    "        \"\"\"\n",
    "        data = next(self._data_loader_iter)\n",
    "        data_time = time.perf_counter() - start\n",
    "\n",
    "        \"\"\"\n",
    "        If your want to do something with the heads, you can wrap the model.\n",
    "        \"\"\"\n",
    "\n",
    "        loss_dict = self.model(data)\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        \"\"\"\n",
    "        If you need accumulate gradients or something similar, you can\n",
    "        wrap the optimizer with your custom `zero_grad()` method.\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        losses.backward()\n",
    "\n",
    "        self._write_metrics(loss_dict, data_time)\n",
    "\n",
    "        \"\"\"\n",
    "        If you need gradient clipping/scaling or other processing, you can\n",
    "        wrap the optimizer with your custom `step()` method.\n",
    "        \"\"\"\n",
    "        self.optimizer.step()\n",
    "        if isinstance(self.param_wrapper, ContiguousParams):\n",
    "            self.param_wrapper.assert_buffer_is_valid()\n",
    "\n",
    "    def _write_metrics(self, loss_dict: Dict[str, torch.Tensor], data_time: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss_dict (dict): dict of scalar losses\n",
    "            data_time (float): time taken by the dataloader iteration\n",
    "        \"\"\"\n",
    "        device = next(iter(loss_dict.values())).device\n",
    "\n",
    "        # Use a new stream so these ops don't wait for DDP or backward\n",
    "        with torch.cuda.stream(torch.cuda.Stream() if device.type == \"cuda\" else None):\n",
    "            metrics_dict = {k: v.detach().cpu().item() for k, v in loss_dict.items()}\n",
    "            metrics_dict[\"data_time\"] = data_time\n",
    "\n",
    "            # Gather metrics among all workers for logging\n",
    "            # This assumes we do DDP-style training, which is currently the only\n",
    "            # supported method in detectron2.\n",
    "            all_metrics_dict = comm.gather(metrics_dict)\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            storage = get_event_storage()\n",
    "\n",
    "            # data_time among workers can have high variance. The actual latency\n",
    "            # caused by data_time is the maximum among workers.\n",
    "            data_time = np.max([x.pop(\"data_time\") for x in all_metrics_dict])\n",
    "            storage.put_scalar(\"data_time\", data_time)\n",
    "\n",
    "            # average the rest metrics\n",
    "            metrics_dict = {\n",
    "                k: np.mean([x[k] for x in all_metrics_dict]) for k in all_metrics_dict[0].keys()\n",
    "            }\n",
    "            total_losses_reduced = sum(metrics_dict.values())\n",
    "            if not np.isfinite(total_losses_reduced):\n",
    "                raise FloatingPointError(\n",
    "                    f\"Loss became infinite or NaN at iteration={self.iter}!\\n\"\n",
    "                    f\"loss_dict = {metrics_dict}\"\n",
    "                )\n",
    "\n",
    "            storage.put_scalar(\"total_loss\", total_losses_reduced)\n",
    "            if len(metrics_dict) > 1:\n",
    "                storage.put_scalars(**metrics_dict)\n",
    "\n",
    "\n",
    "class AMPTrainer(SimpleTrainer):\n",
    "    \"\"\"\n",
    "    Like :class:`SimpleTrainer`, but uses automatic mixed precision\n",
    "    in the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data_loader, optimizer, param_wrapper, grad_scaler=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            model, data_loader, optimizer: same as in :class:`SimpleTrainer`.\n",
    "            grad_scaler: torch GradScaler to automatically scale gradients.\n",
    "        \"\"\"\n",
    "        unsupported = \"AMPTrainer does not support single-process multi-device training!\"\n",
    "        if isinstance(model, DistributedDataParallel):\n",
    "            assert not (model.device_ids and len(model.device_ids) > 1), unsupported\n",
    "        assert not isinstance(model, DataParallel), unsupported\n",
    "\n",
    "        super().__init__(model, data_loader, optimizer, param_wrapper)\n",
    "\n",
    "        if grad_scaler is None:\n",
    "            from torch.cuda.amp import GradScaler\n",
    "\n",
    "            grad_scaler = GradScaler()\n",
    "        self.grad_scaler = grad_scaler\n",
    "\n",
    "    def run_step(self):\n",
    "        \"\"\"\n",
    "        Implement the AMP training logic.\n",
    "        \"\"\"\n",
    "        assert self.model.training, \"[AMPTrainer] model was changed to eval mode!\"\n",
    "        assert torch.cuda.is_available(), \"[AMPTrainer] CUDA is required for AMP training!\"\n",
    "        from torch.cuda.amp import autocast\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        data = next(self._data_loader_iter)\n",
    "        data_time = time.perf_counter() - start\n",
    "\n",
    "        with autocast():\n",
    "            loss_dict = self.model(data)\n",
    "            losses = sum(loss_dict.values())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        self.grad_scaler.scale(losses).backward()\n",
    "\n",
    "        self._write_metrics(loss_dict, data_time)\n",
    "\n",
    "        self.grad_scaler.step(self.optimizer)\n",
    "        self.grad_scaler.update()\n",
    "        if isinstance(self.param_wrapper, ContiguousParams):\n",
    "            self.param_wrapper.assert_buffer_is_valid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a8b8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\n",
    "import datetime\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from fastreid.evaluation.testing import flatten_results_dict\n",
    "import sys\n",
    "sys.path.append(\"C:\\CCTV\\fast-reid\\fastreid\\solver\\optim\")\n",
    "# from fastreid.solver import optim\n",
    "from fastreid.utils import comm\n",
    "from fastreid.utils.checkpoint import PeriodicCheckpointer as _PeriodicCheckpointer\n",
    "from fastreid.utils.events import EventStorage, EventWriter, get_event_storage\n",
    "from fastreid.utils.file_io import PathManager\n",
    "from fastreid.utils.precision_bn import update_bn_stats, get_bn_modules\n",
    "from fastreid.utils.timer import Timer\n",
    "# from .train_loop import HookBase\n",
    "\n",
    "__all__ = [\n",
    "    \"CallbackHook\",\n",
    "    \"IterationTimer\",\n",
    "    \"PeriodicWriter\",\n",
    "    \"PeriodicCheckpointer\",\n",
    "    \"LRScheduler\",\n",
    "    \"AutogradProfiler\",\n",
    "    \"EvalHook\",\n",
    "    \"PreciseBN\",\n",
    "    \"LayerFreeze\",\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Implement some common hooks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CallbackHook(HookBase):\n",
    "    \"\"\"\n",
    "    Create a hook using callback functions provided by the user.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, before_train=None, after_train=None, before_epoch=None, after_epoch=None,\n",
    "                 before_step=None, after_step=None):\n",
    "        \"\"\"\n",
    "        Each argument is a function that takes one argument: the trainer.\n",
    "        \"\"\"\n",
    "        self._before_train = before_train\n",
    "        self._before_epoch = before_epoch\n",
    "        self._before_step = before_step\n",
    "        self._after_step = after_step\n",
    "        self._after_epoch = after_epoch\n",
    "        self._after_train = after_train\n",
    "\n",
    "    def before_train(self):\n",
    "        if self._before_train:\n",
    "            self._before_train(self.trainer)\n",
    "\n",
    "    def after_train(self):\n",
    "        if self._after_train:\n",
    "            self._after_train(self.trainer)\n",
    "        # The functions may be closures that hold reference to the trainer\n",
    "        # Therefore, delete them to avoid circular reference.\n",
    "        del self._before_train, self._after_train\n",
    "        del self._before_step, self._after_step\n",
    "\n",
    "    def before_epoch(self):\n",
    "        if self._before_epoch:\n",
    "            self._before_epoch(self.trainer)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if self._after_epoch:\n",
    "            self._after_epoch(self.trainer)\n",
    "\n",
    "    def before_step(self):\n",
    "        if self._before_step:\n",
    "            self._before_step(self.trainer)\n",
    "\n",
    "    def after_step(self):\n",
    "        if self._after_step:\n",
    "            self._after_step(self.trainer)\n",
    "\n",
    "\n",
    "class IterationTimer(HookBase):\n",
    "    \"\"\"\n",
    "    Track the time spent for each iteration (each run_step call in the trainer).\n",
    "    Print a summary in the end of training.\n",
    "    This hook uses the time between the call to its :meth:`before_step`\n",
    "    and :meth:`after_step` methods.\n",
    "    Under the convention that :meth:`before_step` of all hooks should only\n",
    "    take negligible amount of time, the :class:`IterationTimer` hook should be\n",
    "    placed at the beginning of the list of hooks to obtain accurate timing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, warmup_iter=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            warmup_iter (int): the number of iterations at the beginning to exclude\n",
    "                from timing.\n",
    "        \"\"\"\n",
    "        self._warmup_iter = warmup_iter\n",
    "        self._step_timer = Timer()\n",
    "\n",
    "    def before_train(self):\n",
    "        self._start_time = time.perf_counter()\n",
    "        self._total_timer = Timer()\n",
    "        self._total_timer.pause()\n",
    "\n",
    "    def after_train(self):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        total_time = time.perf_counter() - self._start_time\n",
    "        total_time_minus_hooks = self._total_timer.seconds()\n",
    "        hook_time = total_time - total_time_minus_hooks\n",
    "\n",
    "        num_iter = self.trainer.iter + 1 - self.trainer.start_iter - self._warmup_iter\n",
    "\n",
    "        if num_iter > 0 and total_time_minus_hooks > 0:\n",
    "            # Speed is meaningful only after warmup\n",
    "            # NOTE this format is parsed by grep in some scripts\n",
    "            logger.info(\n",
    "                \"Overall training speed: {} iterations in {} ({:.4f} s / it)\".format(\n",
    "                    num_iter,\n",
    "                    str(datetime.timedelta(seconds=int(total_time_minus_hooks))),\n",
    "                    total_time_minus_hooks / num_iter,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            \"Total training time: {} ({} on hooks)\".format(\n",
    "                str(datetime.timedelta(seconds=int(total_time))),\n",
    "                str(datetime.timedelta(seconds=int(hook_time))),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def before_step(self):\n",
    "        self._step_timer.reset()\n",
    "        self._total_timer.resume()\n",
    "\n",
    "    def after_step(self):\n",
    "        # +1 because we're in after_step\n",
    "        iter_done = self.trainer.iter - self.trainer.start_iter + 1\n",
    "        if iter_done >= self._warmup_iter:\n",
    "            sec = self._step_timer.seconds()\n",
    "            self.trainer.storage.put_scalars(time=sec)\n",
    "        else:\n",
    "            self._start_time = time.perf_counter()\n",
    "            self._total_timer.reset()\n",
    "\n",
    "        self._total_timer.pause()\n",
    "\n",
    "\n",
    "class PeriodicWriter(HookBase):\n",
    "    \"\"\"\n",
    "    Write events to EventStorage periodically.\n",
    "    It is executed every ``period`` iterations and after the last iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, writers, period=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            writers (list[EventWriter]): a list of EventWriter objects\n",
    "            period (int):\n",
    "        \"\"\"\n",
    "        self._writers = writers\n",
    "        for w in writers:\n",
    "            assert isinstance(w, EventWriter), w\n",
    "        self._period = period\n",
    "\n",
    "    def after_step(self):\n",
    "        if (self.trainer.iter + 1) % self._period == 0 or (\n",
    "                self.trainer.iter == self.trainer.max_iter - 1\n",
    "        ):\n",
    "            for writer in self._writers:\n",
    "                writer.write()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        for writer in self._writers:\n",
    "            writer.write()\n",
    "\n",
    "    def after_train(self):\n",
    "        for writer in self._writers:\n",
    "            writer.close()\n",
    "\n",
    "\n",
    "class PeriodicCheckpointer(_PeriodicCheckpointer, HookBase):\n",
    "    \"\"\"\n",
    "    Same as :class:`fastreid.utils.checkpoint.PeriodicCheckpointer`, but as a hook.\n",
    "    Note that when used as a hook,\n",
    "    it is unable to save additional data other than what's defined\n",
    "    by the given `checkpointer`.\n",
    "    It is executed every ``period`` iterations and after the last iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    def before_train(self):\n",
    "        self.max_epoch = self.trainer.max_epoch\n",
    "        if len(self.trainer.cfg.DATASETS.TESTS) == 1:\n",
    "            self.metric_name = \"metric\"\n",
    "        else:\n",
    "            self.metric_name = self.trainer.cfg.DATASETS.TESTS[0] + \"/metric\"\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # No way to use **kwargs\n",
    "        storage = get_event_storage()\n",
    "        metric_dict = dict(\n",
    "            metric=storage.latest()[self.metric_name][0] if self.metric_name in storage.latest() else -1\n",
    "        )\n",
    "        self.step(self.trainer.epoch, **metric_dict)\n",
    "\n",
    "\n",
    "class LRScheduler(HookBase):\n",
    "    \"\"\"\n",
    "    A hook which executes a torch builtin LR scheduler and summarizes the LR.\n",
    "    It is executed after every iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, scheduler):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer):\n",
    "            scheduler (torch.optim._LRScheduler)\n",
    "        \"\"\"\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        self._scale = 0\n",
    "\n",
    "        # NOTE: some heuristics on what LR to summarize\n",
    "        # summarize the param group with most parameters\n",
    "        largest_group = max(len(g[\"params\"]) for g in optimizer.param_groups)\n",
    "\n",
    "        if largest_group == 1:\n",
    "            # If all groups have one parameter,\n",
    "            # then find the most common initial LR, and use it for summary\n",
    "            lr_count = Counter([g[\"lr\"] for g in optimizer.param_groups])\n",
    "            lr = lr_count.most_common()[0][0]\n",
    "            for i, g in enumerate(optimizer.param_groups):\n",
    "                if g[\"lr\"] == lr:\n",
    "                    self._best_param_group_id = i\n",
    "                    break\n",
    "        else:\n",
    "            for i, g in enumerate(optimizer.param_groups):\n",
    "                if len(g[\"params\"]) == largest_group:\n",
    "                    self._best_param_group_id = i\n",
    "                    break\n",
    "\n",
    "    def before_step(self):\n",
    "        if self.trainer.grad_scaler is not None:\n",
    "            self._scale = self.trainer.grad_scaler.get_scale()\n",
    "\n",
    "    def after_step(self):\n",
    "        lr = self._optimizer.param_groups[self._best_param_group_id][\"lr\"]\n",
    "        self.trainer.storage.put_scalar(\"lr\", lr, smoothing_hint=False)\n",
    "\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        if next_iter <= self.trainer.warmup_iters:\n",
    "            if self.trainer.grad_scaler is None or self._scale == self.trainer.grad_scaler.get_scale():\n",
    "                self._scheduler[\"warmup_sched\"].step()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        next_epoch = self.trainer.epoch + 1\n",
    "        if next_iter > self.trainer.warmup_iters and next_epoch > self.trainer.delay_epochs:\n",
    "            self._scheduler[\"lr_sched\"].step()\n",
    "\n",
    "\n",
    "class AutogradProfiler(HookBase):\n",
    "    \"\"\"\n",
    "    A hook which runs `torch.autograd.profiler.profile`.\n",
    "    Examples:\n",
    "    .. code-block:: python\n",
    "        hooks.AutogradProfiler(\n",
    "             lambda trainer: trainer.iter > 10 and trainer.iter < 20, self.cfg.OUTPUT_DIR\n",
    "        )\n",
    "    The above example will run the profiler for iteration 10~20 and dump\n",
    "    results to ``OUTPUT_DIR``. We did not profile the first few iterations\n",
    "    because they are typically slower than the rest.\n",
    "    The result files can be loaded in the ``chrome://tracing`` page in chrome browser.\n",
    "    Note:\n",
    "        When used together with NCCL on older version of GPUs,\n",
    "        autograd profiler may cause deadlock because it unnecessarily allocates\n",
    "        memory on every device it sees. The memory management calls, if\n",
    "        interleaved with NCCL calls, lead to deadlock on GPUs that do not\n",
    "        support `cudaLaunchCooperativeKernelMultiDevice`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, enable_predicate, output_dir, *, use_cuda=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            enable_predicate (callable[trainer -> bool]): a function which takes a trainer,\n",
    "                and returns whether to enable the profiler.\n",
    "                It will be called once every step, and can be used to select which steps to profile.\n",
    "            output_dir (str): the output directory to dump tracing files.\n",
    "            use_cuda (bool): same as in `torch.autograd.profiler.profile`.\n",
    "        \"\"\"\n",
    "        self._enable_predicate = enable_predicate\n",
    "        self._use_cuda = use_cuda\n",
    "        self._output_dir = output_dir\n",
    "\n",
    "    def before_step(self):\n",
    "        if self._enable_predicate(self.trainer):\n",
    "            self._profiler = torch.autograd.profiler.profile(use_cuda=self._use_cuda)\n",
    "            self._profiler.__enter__()\n",
    "        else:\n",
    "            self._profiler = None\n",
    "\n",
    "    def after_step(self):\n",
    "        if self._profiler is None:\n",
    "            return\n",
    "        self._profiler.__exit__(None, None, None)\n",
    "        out_file = os.path.join(\n",
    "            self._output_dir, \"profiler-trace-iter{}.json\".format(self.trainer.iter)\n",
    "        )\n",
    "        if \"://\" not in out_file:\n",
    "            self._profiler.export_chrome_trace(out_file)\n",
    "        else:\n",
    "            # Support non-posix filesystems\n",
    "            with tempfile.TemporaryDirectory(prefix=\"fastreid_profiler\") as d:\n",
    "                tmp_file = os.path.join(d, \"tmp.json\")\n",
    "                self._profiler.export_chrome_trace(tmp_file)\n",
    "                with open(tmp_file) as f:\n",
    "                    content = f.read()\n",
    "            with PathManager.open(out_file, \"w\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "\n",
    "class EvalHook(HookBase):\n",
    "    \"\"\"\n",
    "    Run an evaluation function periodically, and at the end of training.\n",
    "    It is executed every ``eval_period`` iterations and after the last iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_period, eval_function):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eval_period (int): the period to run `eval_function`.\n",
    "            eval_function (callable): a function which takes no arguments, and\n",
    "                returns a nested dict of evaluation metrics.\n",
    "        Note:\n",
    "            This hook must be enabled in all or none workers.\n",
    "            If you would like only certain workers to perform evaluation,\n",
    "            give other workers a no-op function (`eval_function=lambda: None`).\n",
    "        \"\"\"\n",
    "        self._period = eval_period\n",
    "        self._func = eval_function\n",
    "\n",
    "    def _do_eval(self):\n",
    "        results = self._func()\n",
    "\n",
    "        if results:\n",
    "            assert isinstance(\n",
    "                results, dict\n",
    "            ), \"Eval function must return a dict. Got {} instead.\".format(results)\n",
    "\n",
    "            flattened_results = flatten_results_dict(results)\n",
    "            for k, v in flattened_results.items():\n",
    "                try:\n",
    "                    v = float(v)\n",
    "                except Exception:\n",
    "                    raise ValueError(\n",
    "                        \"[EvalHook] eval_function should return a nested dict of float. \"\n",
    "                        \"Got '{}: {}' instead.\".format(k, v)\n",
    "                    )\n",
    "            self.trainer.storage.put_scalars(**flattened_results, smoothing_hint=False)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # Evaluation may take different time among workers.\n",
    "        # A barrier make them start the next iteration together.\n",
    "        comm.synchronize()\n",
    "\n",
    "    def after_epoch(self):\n",
    "        next_epoch = self.trainer.epoch + 1\n",
    "        if self._period > 0 and next_epoch % self._period == 0:\n",
    "            self._do_eval()\n",
    "\n",
    "    def after_train(self):\n",
    "        next_epoch = self.trainer.epoch + 1\n",
    "        # This condition is to prevent the eval from running after a failed training\n",
    "        if next_epoch % self._period != 0 and next_epoch >= self.trainer.max_epoch:\n",
    "            self._do_eval()\n",
    "        # func is likely a closure that holds reference to the trainer\n",
    "        # therefore we clean it to avoid circular reference in the end\n",
    "        del self._func\n",
    "\n",
    "\n",
    "class PreciseBN(HookBase):\n",
    "    \"\"\"\n",
    "    The standard implementation of BatchNorm uses EMA in inference, which is\n",
    "    sometimes suboptimal.\n",
    "    This class computes the true average of statistics rather than the moving average,\n",
    "    and put true averages to every BN layer in the given model.\n",
    "    It is executed after the last iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data_loader, num_iter):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): a module whose all BN layers in training mode will be\n",
    "                updated by precise BN.\n",
    "                Note that user is responsible for ensuring the BN layers to be\n",
    "                updated are in training mode when this hook is triggered.\n",
    "            data_loader (iterable): it will produce data to be run by `model(data)`.\n",
    "            num_iter (int): number of iterations used to compute the precise\n",
    "                statistics.\n",
    "        \"\"\"\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        if len(get_bn_modules(model)) == 0:\n",
    "            self._logger.info(\n",
    "                \"PreciseBN is disabled because model does not contain BN layers in training mode.\"\n",
    "            )\n",
    "            self._disabled = True\n",
    "            return\n",
    "\n",
    "        self._model = model\n",
    "        self._data_loader = data_loader\n",
    "        self._num_iter = num_iter\n",
    "        self._disabled = False\n",
    "\n",
    "        self._data_iter = None\n",
    "\n",
    "    def after_epoch(self):\n",
    "        next_epoch = self.trainer.epoch + 1\n",
    "        is_final = next_epoch == self.trainer.max_epoch\n",
    "        if is_final:\n",
    "            self.update_stats()\n",
    "\n",
    "    def update_stats(self):\n",
    "        \"\"\"\n",
    "        Update the model with precise statistics. Users can manually call this method.\n",
    "        \"\"\"\n",
    "        if self._disabled:\n",
    "            return\n",
    "\n",
    "        if self._data_iter is None:\n",
    "            self._data_iter = iter(self._data_loader)\n",
    "\n",
    "        def data_loader():\n",
    "            for num_iter in itertools.count(1):\n",
    "                if num_iter % 100 == 0:\n",
    "                    self._logger.info(\n",
    "                        \"Running precise-BN ... {}/{} iterations.\".format(num_iter, self._num_iter)\n",
    "                    )\n",
    "                # This way we can reuse the same iterator\n",
    "                yield next(self._data_iter)\n",
    "\n",
    "        with EventStorage():  # capture events in a new storage to discard them\n",
    "            self._logger.info(\n",
    "                \"Running precise-BN for {} iterations...  \".format(self._num_iter)\n",
    "                + \"Note that this could produce different statistics every time.\"\n",
    "            )\n",
    "            update_bn_stats(self._model, data_loader(), self._num_iter)\n",
    "\n",
    "\n",
    "class LayerFreeze(HookBase):\n",
    "    def __init__(self, model, freeze_layers, freeze_iters):\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        if isinstance(model, DistributedDataParallel):\n",
    "            model = model.module\n",
    "        self.model = model\n",
    "\n",
    "        self.freeze_layers = freeze_layers\n",
    "        self.freeze_iters = freeze_iters\n",
    "\n",
    "        self.is_frozen = False\n",
    "\n",
    "    def before_step(self):\n",
    "        # Freeze specific layers\n",
    "        if self.trainer.iter < self.freeze_iters and not self.is_frozen:\n",
    "            self.freeze_specific_layer()\n",
    "\n",
    "        # Recover original layers status\n",
    "        if self.trainer.iter >= self.freeze_iters and self.is_frozen:\n",
    "            self.open_all_layer()\n",
    "\n",
    "    def freeze_specific_layer(self):\n",
    "        for layer in self.freeze_layers:\n",
    "            if not hasattr(self.model, layer):\n",
    "                self._logger.info(f'{layer} is not an attribute of the model, will skip this layer')\n",
    "\n",
    "        for name, module in self.model.named_children():\n",
    "            if name in self.freeze_layers:\n",
    "                # Change BN in freeze layers to eval mode\n",
    "                module.eval()\n",
    "\n",
    "        self.is_frozen = True\n",
    "        freeze_layers = \", \".join(self.freeze_layers)\n",
    "        self._logger.info(f'Freeze layer group \"{freeze_layers}\" training for {self.freeze_iters:d} iterations')\n",
    "\n",
    "    def open_all_layer(self):\n",
    "        for name, module in self.model.named_children():\n",
    "            if name in self.freeze_layers:\n",
    "                module.train()\n",
    "\n",
    "        self.is_frozen = False\n",
    "\n",
    "        freeze_layers = \", \".join(self.freeze_layers)\n",
    "        self._logger.info(f'Open layer group \"{freeze_layers}\" training')\n",
    "\n",
    "\n",
    "class SWA(HookBase):\n",
    "    def __init__(self, swa_start: int, swa_freq: int, swa_lr_factor: float, eta_min: float, lr_sched=False, ):\n",
    "        self.swa_start = swa_start\n",
    "        self.swa_freq = swa_freq\n",
    "        self.swa_lr_factor = swa_lr_factor\n",
    "        self.eta_min = eta_min\n",
    "        self.lr_sched = lr_sched\n",
    "\n",
    "    def before_step(self):\n",
    "        is_swa = self.trainer.iter == self.swa_start\n",
    "        if is_swa:\n",
    "            # Wrapper optimizer with SWA\n",
    "            self.trainer.optimizer = optim.SWA(self.trainer.optimizer, self.swa_freq, self.swa_lr_factor)\n",
    "            self.trainer.optimizer.reset_lr_to_swa()\n",
    "\n",
    "            if self.lr_sched:\n",
    "                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                    optimizer=self.trainer.optimizer,\n",
    "                    T_0=self.swa_freq,\n",
    "                    eta_min=self.eta_min,\n",
    "                )\n",
    "\n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "\n",
    "        # Use Cyclic learning rate scheduler\n",
    "        if next_iter > self.swa_start and self.lr_sched:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final:\n",
    "            self.trainer.optimizer.swap_swa_param()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2451225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\n",
    "\"\"\"\n",
    "This file contains components with some default boilerplate logic user may need\n",
    "in training / testing. They will not work for everyone, but many users may find them useful.\n",
    "The behavior of functions/classes in this file is subject to change,\n",
    "since they are meant to represent the \"common default behavior\" people need in their projects.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from fastreid.data import build_reid_test_loader, build_reid_train_loader\n",
    "from fastreid.evaluation import (ReidEvaluator,\n",
    "                                 inference_on_dataset, print_csv_format)\n",
    "from fastreid.modeling.meta_arch import build_model\n",
    "from fastreid.solver import build_lr_scheduler, build_optimizer\n",
    "from fastreid.utils import comm\n",
    "from fastreid.utils.checkpoint import Checkpointer\n",
    "from fastreid.utils.collect_env import collect_env_info\n",
    "from fastreid.utils.env import seed_all_rng\n",
    "from fastreid.utils.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter\n",
    "from fastreid.utils.file_io import PathManager\n",
    "from fastreid.utils.logger import setup_logger\n",
    "# from . import hooks\n",
    "# from .train_loop import TrainerBase, AMPTrainer, SimpleTrainer\n",
    "\n",
    "__all__ = [\"default_argument_parser\", \"default_setup\", \"DefaultPredictor\", \"DefaultTrainer\"]\n",
    "\n",
    "\n",
    "def default_argument_parser():\n",
    "    \"\"\"\n",
    "    Create a parser with some common arguments used by fastreid users.\n",
    "    Returns:\n",
    "        argparse.ArgumentParser:\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"fastreid Training\")\n",
    "    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n",
    "    parser.add_argument(\n",
    "        \"--resume\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to attempt to resume from the checkpoint directory\",\n",
    "    )\n",
    "    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=1, help=\"number of gpus *per machine*\")\n",
    "    parser.add_argument(\"--num-machines\", type=int, default=1, help=\"total number of machines\")\n",
    "    parser.add_argument(\n",
    "        \"--machine-rank\", type=int, default=0, help=\"the rank of this machine (unique per machine)\"\n",
    "    )\n",
    "\n",
    "    # PyTorch still may leave orphan processes in multi-gpu training.\n",
    "    # Therefore we use a deterministic way to obtain port,\n",
    "    # so that users are aware of orphan processes by seeing the port occupied.\n",
    "    port = 2 ** 15 + 2 ** 14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2 ** 14\n",
    "    parser.add_argument(\"--dist-url\", default=\"tcp://127.0.0.1:{}\".format(port))\n",
    "    parser.add_argument(\n",
    "        \"opts\",\n",
    "        help=\"Modify config options using the command-line\",\n",
    "        default=None,\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def default_setup(cfg, args):\n",
    "    \"\"\"\n",
    "    Perform some basic common setups at the beginning of a job, including:\n",
    "    1. Set up the detectron2 logger\n",
    "    2. Log basic information about environment, cmdline arguments, and config\n",
    "    3. Backup the config to the output directory\n",
    "    Args:\n",
    "        cfg (CfgNode): the full config to be used\n",
    "        args (argparse.NameSpace): the command line arguments to be logged\n",
    "    \"\"\"\n",
    "    output_dir = cfg.OUTPUT_DIR\n",
    "    if comm.is_main_process() and output_dir:\n",
    "        PathManager.mkdirs(output_dir)\n",
    "\n",
    "    rank = comm.get_rank()\n",
    "    # setup_logger(output_dir, distributed_rank=rank, name=\"fvcore\")\n",
    "    logger = setup_logger(output_dir, distributed_rank=rank)\n",
    "\n",
    "    logger.info(\"Rank of current process: {}. World size: {}\".format(rank, comm.get_world_size()))\n",
    "    logger.info(\"Environment info:\\n\" + collect_env_info())\n",
    "\n",
    "    logger.info(\"Command line arguments: \" + str(args))\n",
    "    if hasattr(args, \"config_file\") and args.config_file != \"\":\n",
    "        logger.info(\n",
    "            \"Contents of args.config_file={}:\\n{}\".format(\n",
    "                args.config_file, PathManager.open(args.config_file, \"r\").read()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    logger.info(\"Running with full config:\\n{}\".format(cfg))\n",
    "    if comm.is_main_process() and output_dir:\n",
    "        # Note: some of our scripts may expect the existence of\n",
    "        # config.yaml in output directory\n",
    "        path = os.path.join(output_dir, \"config.yaml\")\n",
    "        with PathManager.open(path, \"w\") as f:\n",
    "            f.write(cfg.dump())\n",
    "        logger.info(\"Full config saved to {}\".format(os.path.abspath(path)))\n",
    "\n",
    "    # make sure each worker has a different, yet deterministic seed if specified\n",
    "    seed_all_rng()\n",
    "\n",
    "    # cudnn benchmark has large overhead. It shouldn't be used considering the small size of\n",
    "    # typical validation set.\n",
    "    if not (hasattr(args, \"eval_only\") and args.eval_only):\n",
    "        torch.backends.cudnn.benchmark = cfg.CUDNN_BENCHMARK\n",
    "\n",
    "\n",
    "class DefaultPredictor:\n",
    "    \"\"\"\n",
    "    Create a simple end-to-end predictor with the given config.\n",
    "    The predictor takes an BGR image, resizes it to the specified resolution,\n",
    "    runs the model and produces a dict of predictions.\n",
    "    This predictor takes care of model loading and input preprocessing for you.\n",
    "    If you'd like to do anything more fancy, please refer to its source code\n",
    "    as examples to build and use the model manually.\n",
    "    Attributes:\n",
    "    Examples:\n",
    "    .. code-block:: python\n",
    "        pred = DefaultPredictor(cfg)\n",
    "        inputs = cv2.imread(\"input.jpg\")\n",
    "        outputs = pred(inputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.cfg.defrost()\n",
    "        self.cfg.MODEL.BACKBONE.PRETRAIN = False\n",
    "        self.model = build_model(self.cfg)\n",
    "        self.model.eval()\n",
    "\n",
    "        Checkpointer(self.model).load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (torch.tensor): an image tensor of shape (B, C, H, W).\n",
    "        Returns:\n",
    "            predictions (torch.tensor): the output features of the model\n",
    "        \"\"\"\n",
    "        inputs = {\"images\": image.to(self.model.device)}\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            predictions = self.model(inputs)\n",
    "        return predictions.cpu()\n",
    "\n",
    "\n",
    "class DefaultTrainer(TrainerBase):\n",
    "    \"\"\"\n",
    "    A trainer with default training logic. Compared to `SimpleTrainer`, it\n",
    "    contains the following logic in addition:\n",
    "    1. Create model, optimizer, scheduler, dataloader from the given config.\n",
    "    2. Load a checkpoint or `cfg.MODEL.WEIGHTS`, if exists.\n",
    "    3. Register a few common hooks.\n",
    "    It is created to simplify the **standard model training workflow** and reduce code boilerplate\n",
    "    for users who only need the standard training workflow, with standard features.\n",
    "    It means this class makes *many assumptions* about your training logic that\n",
    "    may easily become invalid in a new research. In fact, any assumptions beyond those made in the\n",
    "    :class:`SimpleTrainer` are too much for research.\n",
    "    The code of this class has been annotated about restrictive assumptions it mades.\n",
    "    When they do not work for you, you're encouraged to:\n",
    "    1. Overwrite methods of this class, OR:\n",
    "    2. Use :class:`SimpleTrainer`, which only does minimal SGD training and\n",
    "       nothing else. You can then add your own hooks if needed. OR:\n",
    "    3. Write your own training loop similar to `tools/plain_train_net.py`.\n",
    "    Also note that the behavior of this class, like other functions/classes in\n",
    "    this file, is not stable, since it is meant to represent the \"common default behavior\".\n",
    "    It is only guaranteed to work well with the standard models and training workflow in fastreid.\n",
    "    To obtain more stable behavior, write your own training logic with other public APIs.\n",
    "    Attributes:\n",
    "        scheduler:\n",
    "        checkpointer:\n",
    "        cfg (CfgNode):\n",
    "    Examples:\n",
    "    .. code-block:: python\n",
    "        trainer = DefaultTrainer(cfg)\n",
    "        trainer.resume_or_load()  # load last checkpoint or MODEL.WEIGHTS\n",
    "        trainer.train()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        logger = logging.getLogger(\"fastreid\")\n",
    "        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for fastreid\n",
    "            setup_logger()\n",
    "\n",
    "        # Assume these objects must be constructed in this order.\n",
    "        data_loader = self.build_train_loader(cfg)\n",
    "        cfg = self.auto_scale_hyperparams(cfg, data_loader.dataset.num_classes)\n",
    "        model = self.build_model(cfg)\n",
    "        optimizer, param_wrapper = self.build_optimizer(cfg, model)\n",
    "\n",
    "        # For training, wrap with DDP. But don't need this for inference.\n",
    "        if comm.get_world_size() > 1:\n",
    "            # ref to https://github.com/pytorch/pytorch/issues/22049 to set `find_unused_parameters=True`\n",
    "            # for part of the parameters is not updated.\n",
    "            model = DistributedDataParallel(\n",
    "                model, device_ids=[comm.get_local_rank()], broadcast_buffers=False,\n",
    "            )\n",
    "\n",
    "        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n",
    "            model, data_loader, optimizer, param_wrapper\n",
    "        )\n",
    "\n",
    "        self.iters_per_epoch = len(data_loader.dataset) // cfg.SOLVER.IMS_PER_BATCH\n",
    "        self.scheduler = self.build_lr_scheduler(cfg, optimizer, self.iters_per_epoch)\n",
    "\n",
    "        # Assume no other objects need to be checkpointed.\n",
    "        # We can later make it checkpoint the stateful hooks\n",
    "        self.checkpointer = Checkpointer(\n",
    "            # Assume you want to save checkpoints together with logs/statistics\n",
    "            model,\n",
    "            cfg.OUTPUT_DIR,\n",
    "            save_to_disk=comm.is_main_process(),\n",
    "            optimizer=optimizer,\n",
    "            **self.scheduler,\n",
    "        )\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.max_epoch = cfg.SOLVER.MAX_EPOCH\n",
    "        self.max_iter = self.max_epoch * self.iters_per_epoch\n",
    "        self.warmup_iters = cfg.SOLVER.WARMUP_ITERS\n",
    "        self.delay_epochs = cfg.SOLVER.DELAY_EPOCHS\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.register_hooks(self.build_hooks())\n",
    "\n",
    "    def resume_or_load(self, resume=True):\n",
    "        \"\"\"\n",
    "        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n",
    "        a `last_checkpoint` file), resume from the file. Resuming means loading all\n",
    "        available states (eg. optimizer and scheduler) and update iteration counter\n",
    "        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n",
    "        Otherwise, this is considered as an independent training. The method will load model\n",
    "        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n",
    "        from iteration 0.\n",
    "        Args:\n",
    "            resume (bool): whether to do resume or not\n",
    "        \"\"\"\n",
    "        # The checkpoint stores the training iteration that just finished, thus we start\n",
    "        # at the next iteration (or iter zero if there's no checkpoint).\n",
    "        checkpoint = self.checkpointer.resume_or_load(self.cfg.MODEL.WEIGHTS, resume=resume)\n",
    "\n",
    "        if resume and self.checkpointer.has_checkpoint():\n",
    "            self.start_epoch = checkpoint.get(\"epoch\", -1) + 1\n",
    "            # The checkpoint stores the training iteration that just finished, thus we start\n",
    "            # at the next iteration (or iter zero if there's no checkpoint).\n",
    "\n",
    "    def build_hooks(self):\n",
    "        \"\"\"\n",
    "        Build a list of default hooks, including timing, evaluation,\n",
    "        checkpointing, lr scheduling, precise BN, writing events.\n",
    "        Returns:\n",
    "            list[HookBase]:\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        cfg = self.cfg.clone()\n",
    "        cfg.defrost()\n",
    "        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n",
    "        cfg.DATASETS.NAMES = tuple([cfg.TEST.PRECISE_BN.DATASET])  # set dataset name for PreciseBN\n",
    "\n",
    "        ret = [\n",
    "            hooks.IterationTimer(),\n",
    "            hooks.LRScheduler(self.optimizer, self.scheduler),\n",
    "        ]\n",
    "\n",
    "        if cfg.TEST.PRECISE_BN.ENABLED and hooks.get_bn_modules(self.model):\n",
    "            logger.info(\"Prepare precise BN dataset\")\n",
    "            ret.append(hooks.PreciseBN(\n",
    "                # Run at the same freq as (but before) evaluation.\n",
    "                self.model,\n",
    "                # Build a new data loader to not affect training\n",
    "                self.build_train_loader(cfg),\n",
    "                cfg.TEST.PRECISE_BN.NUM_ITER,\n",
    "            ))\n",
    "\n",
    "        if len(cfg.MODEL.FREEZE_LAYERS) > 0 and cfg.SOLVER.FREEZE_ITERS > 0:\n",
    "            ret.append(hooks.LayerFreeze(\n",
    "                self.model,\n",
    "                cfg.MODEL.FREEZE_LAYERS,\n",
    "                cfg.SOLVER.FREEZE_ITERS,\n",
    "            ))\n",
    "\n",
    "        # Do PreciseBN before checkpointer, because it updates the model and need to\n",
    "        # be saved by checkpointer.\n",
    "        # This is not always the best: if checkpointing has a different frequency,\n",
    "        # some checkpoints may have more precise statistics than others.\n",
    "\n",
    "        def test_and_save_results():\n",
    "            self._last_eval_results = self.test(self.cfg, self.model)\n",
    "            return self._last_eval_results\n",
    "\n",
    "        # Do evaluation before checkpointer, because then if it fails,\n",
    "        # we can use the saved checkpoint to debug.\n",
    "        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))\n",
    "            # run writers in the end, so that evaluation metrics are written\n",
    "            ret.append(hooks.PeriodicWriter(self.build_writers(), 200))\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def build_writers(self):\n",
    "        \"\"\"\n",
    "        Build a list of writers to be used. By default it contains\n",
    "        writers that write metrics to the screen,\n",
    "        a json file, and a tensorboard event file respectively.\n",
    "        If you'd like a different list of writers, you can overwrite it in\n",
    "        your trainer.\n",
    "        Returns:\n",
    "            list[EventWriter]: a list of :class:`EventWriter` objects.\n",
    "        It is now implemented by:\n",
    "        .. code-block:: python\n",
    "            return [\n",
    "                CommonMetricPrinter(self.max_iter),\n",
    "                JSONWriter(os.path.join(self.cfg.OUTPUT_DIR, \"metrics.json\")),\n",
    "                TensorboardXWriter(self.cfg.OUTPUT_DIR),\n",
    "            ]\n",
    "        \"\"\"\n",
    "        # Assume the default print/log frequency.\n",
    "        return [\n",
    "            # It may not always print what you want to see, since it prints \"common\" metrics only.\n",
    "            CommonMetricPrinter(self.max_iter),\n",
    "            JSONWriter(os.path.join(self.cfg.OUTPUT_DIR, \"metrics.json\")),\n",
    "            TensorboardXWriter(self.cfg.OUTPUT_DIR),\n",
    "        ]\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Run training.\n",
    "        Returns:\n",
    "            OrderedDict of results, if evaluation is enabled. Otherwise None.\n",
    "        \"\"\"\n",
    "        super().train(self.start_epoch, self.max_epoch, self.iters_per_epoch)\n",
    "        if comm.is_main_process():\n",
    "            assert hasattr(\n",
    "                self, \"_last_eval_results\"\n",
    "            ), \"No evaluation results obtained during training!\"\n",
    "            return self._last_eval_results\n",
    "\n",
    "    def run_step(self):\n",
    "        self._trainer.iter = self.iter\n",
    "        self._trainer.run_step()\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, cfg):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            torch.nn.Module:\n",
    "        It now calls :func:`fastreid.modeling.build_model`.\n",
    "        Overwrite it if you'd like a different model.\n",
    "        \"\"\"\n",
    "        model = build_model(cfg)\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Model:\\n{}\".format(model))\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def build_optimizer(cls, cfg, model):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            torch.optim.Optimizer:\n",
    "        It now calls :func:`fastreid.solver.build_optimizer`.\n",
    "        Overwrite it if you'd like a different optimizer.\n",
    "        \"\"\"\n",
    "        return build_optimizer(cfg, model)\n",
    "\n",
    "    @classmethod\n",
    "    def build_lr_scheduler(cls, cfg, optimizer, iters_per_epoch):\n",
    "        \"\"\"\n",
    "        It now calls :func:`fastreid.solver.build_lr_scheduler`.\n",
    "        Overwrite it if you'd like a different scheduler.\n",
    "        \"\"\"\n",
    "        return build_lr_scheduler(cfg, optimizer, iters_per_epoch)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            iterable\n",
    "        It now calls :func:`fastreid.data.build_reid_train_loader`.\n",
    "        Overwrite it if you'd like a different data loader.\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Prepare training set\")\n",
    "        return build_reid_train_loader(cfg, combineall=cfg.DATASETS.COMBINEALL)\n",
    "\n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            iterable\n",
    "        It now calls :func:`fastreid.data.build_reid_test_loader`.\n",
    "        Overwrite it if you'd like a different data loader.\n",
    "        \"\"\"\n",
    "        return build_reid_test_loader(cfg, dataset_name=dataset_name)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_dir=None):\n",
    "        data_loader, num_query = cls.build_test_loader(cfg, dataset_name)\n",
    "        return data_loader, ReidEvaluator(cfg, num_query, output_dir)\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, cfg, model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "            model (nn.Module):\n",
    "        Returns:\n",
    "            dict: a dict of result metrics\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        results = OrderedDict()\n",
    "        for idx, dataset_name in enumerate(cfg.DATASETS.TESTS):\n",
    "            logger.info(\"Prepare testing set\")\n",
    "            try:\n",
    "                data_loader, evaluator = cls.build_evaluator(cfg, dataset_name)\n",
    "            except NotImplementedError:\n",
    "                logger.warn(\n",
    "                    \"No evaluator found. implement its `build_evaluator` method.\"\n",
    "                )\n",
    "                results[dataset_name] = {}\n",
    "                continue\n",
    "            results_i = inference_on_dataset(model, data_loader, evaluator, flip_test=cfg.TEST.FLIP.ENABLED)\n",
    "            results[dataset_name] = results_i\n",
    "\n",
    "            if comm.is_main_process():\n",
    "                assert isinstance(\n",
    "                    results, dict\n",
    "                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n",
    "                    results\n",
    "                )\n",
    "                logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n",
    "                results_i['dataset'] = dataset_name\n",
    "                print_csv_format(results_i)\n",
    "\n",
    "        if len(results) == 1:\n",
    "            results = list(results.values())[0]\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def auto_scale_hyperparams(cfg, num_classes):\n",
    "        r\"\"\"\n",
    "        This is used for auto-computation actual training iterations,\n",
    "        because some hyper-param, such as MAX_ITER, means training epochs rather than iters,\n",
    "        so we need to convert specific hyper-param to training iterations.\n",
    "        \"\"\"\n",
    "        cfg = cfg.clone()\n",
    "        frozen = cfg.is_frozen()\n",
    "        cfg.defrost()\n",
    "\n",
    "        # If you don't hard-code the number of classes, it will compute the number automatically\n",
    "        if cfg.MODEL.HEADS.NUM_CLASSES == 0:\n",
    "            output_dir = cfg.OUTPUT_DIR\n",
    "            cfg.MODEL.HEADS.NUM_CLASSES = num_classes\n",
    "            logger = logging.getLogger(__name__)\n",
    "            logger.info(f\"Auto-scaling the num_classes={cfg.MODEL.HEADS.NUM_CLASSES}\")\n",
    "\n",
    "            # Update the saved config file to make the number of classes valid\n",
    "            if comm.is_main_process() and output_dir:\n",
    "                # Note: some of our scripts may expect the existence of\n",
    "                # config.yaml in output directory\n",
    "                path = os.path.join(output_dir, \"config.yaml\")\n",
    "                with PathManager.open(path, \"w\") as f:\n",
    "                    f.write(cfg.dump())\n",
    "\n",
    "        if frozen: cfg.freeze()\n",
    "\n",
    "        return cfg\n",
    "\n",
    "\n",
    "# Access basic attributes from the underlying trainer\n",
    "for _attr in [\"model\", \"data_loader\", \"optimizer\", \"grad_scaler\"]:\n",
    "    setattr(DefaultTrainer, _attr, property(lambda self, x=_attr: getattr(self._trainer, x, None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b96b1cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastreid.modeling import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e3ca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# from fastreid.modeling import build_model\n",
    "# # Define transformation to tensor and normalization (based on your ReID model's requirements)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((256, 128)),  # standard size for ReID models like FastReID\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "# ])\n",
    "\n",
    "# import cv2 as cv\n",
    "# import time\n",
    "# import torch\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # --- FastReID Setup ---\n",
    "# from fastreid.config import get_cfg\n",
    "# # from fastreid.engine import DefaultPredictor\n",
    "\n",
    "# def setup_reid_predictor():\n",
    "#     cfg = get_cfg()\n",
    "#     cfg.merge_from_file(\"C:/CCTV/fast-reid/configs/Market1501/bagtricks_R50.yml\")\n",
    "#     cfg.MODEL.WEIGHTS = \"C:/CCTV/market_bot_R50.pth\" # The model you downloaded\n",
    "#     cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     cfg.freeze()\n",
    "#     return (cfg)\n",
    "\n",
    "# # --- Initialize Models and Identity Management ---\n",
    "# model = YOLO(\"yolov8n.pt\") \n",
    "# reid_predictor = setup_reid_predictor()\n",
    "# rmodel = build_model(reid_predictor)\n",
    "# Checkpointer(rmodel).load(reid_predictor.MODEL.WEIGHTS)\n",
    "# rmodel.eval()\n",
    "# # This is our \"memory\" of who we've seen\n",
    "# tracked_identities = {}\n",
    "# next_permanent_id = 0\n",
    "# similarity_threshold = 0.85\n",
    "\n",
    "# ptime = 0\n",
    "\n",
    "# # Use a basic tracker here; we will handle the ReID part ourselves\n",
    "# for result in model.track(source=\"palace.mp4\", stream=True, tracker=\"bytetrack.yaml\", classes=[0]):\n",
    "#     frame = result.orig_img\n",
    "    \n",
    "#     if result.boxes.id is not None:\n",
    "#         boxes = result.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "#         for box in boxes:\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "            \n",
    "#             # Crop the detected person\n",
    "#             crop = frame[y1:y2, x1:x2]\n",
    "#             if crop.size == 0: continue\n",
    "#             tensor_crop = transform(crop).unsqueeze(0).to(reid_predictor.MODEL.DEVICE) \n",
    "#             # Extract visual features\n",
    "#             rmodel.eval()\n",
    "#             features = rmodel(tensor_crop)[0]\n",
    "            \n",
    "#             # --- Matching Logic ---\n",
    "#             best_match_id = -1\n",
    "#             max_similarity = -1\n",
    "\n",
    "#             # Compare the new features to all known identities\n",
    "#             for pid, stored_feature in tracked_identities.items():\n",
    "#                 similarity = torch.nn.functional.cosine_similarity(features.unsqueeze(0), stored_feature.unsqueeze(0)).item()\n",
    "#                 if similarity > max_similarity:\n",
    "#                     max_similarity = similarity\n",
    "#                     best_match_id = pid\n",
    "            \n",
    "#             final_id = -1\n",
    "#             if max_similarity > similarity_threshold:\n",
    "#                 final_id = best_match_id # It's a match!\n",
    "#                 tracked_identities[final_id] = features # Update features\n",
    "#             else:\n",
    "#                 final_id = next_permanent_id # It's a new person\n",
    "                \n",
    "#                 tracked_identities[final_id] = features\n",
    "#                 next_permanent_id += 1\n",
    "#             print(len(tracked_identities),tracked_identities)\n",
    "\n",
    "#             # --- Drawing Logic ---\n",
    "#             label = f\"Person ID: {final_id}\"\n",
    "#             cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#             cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # FPS Calculation\n",
    "#     ctime = time.time()\n",
    "#     if ptime != 0:\n",
    "#         fps = 1 / (ctime - ptime)\n",
    "#         cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "#     ptime = ctime\n",
    "\n",
    "#     cv.imshow(\"Manual ReID Tracking\", frame)\n",
    "#     if cv.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8089957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "# import cv2 as cv\n",
    "# import time\n",
    "# import torch\n",
    "# from ultralytics import YOLO\n",
    "# from fastreid.config import get_cfg\n",
    "\n",
    "\n",
    "# # --- Transformations (You may not need this if using DefaultPredictor directly) ---\n",
    "# # DefaultPredictor handles resizing and normalization internally.\n",
    "# # This manual transform is good for custom models but might be redundant here.\n",
    "# # transform = transforms.Compose([...])\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((256, 128)),  # Standard input size for Re-ID models\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "# def setup_reid_predictor():\n",
    "#     cfg = get_cfg()\n",
    "#     cfg.merge_from_file(\"fast-reid/configs/Market1501/bagtricks_R50.yml\")\n",
    "#     cfg.MODEL.WEIGHTS = \"market_bot_R50.pth\"\n",
    "#     cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     cfg.freeze()\n",
    "#     return (cfg)\n",
    "\n",
    "# # --- Initialize ---\n",
    "# model = YOLO(\"yolov8n.pt\") \n",
    "# reid_predictor = setup_reid_predictor()\n",
    "# rmodel = build_model(reid_predictor)\n",
    "# Checkpointer(rmodel).load(reid_predictor.MODEL.WEIGHTS)\n",
    "# rmodel.eval()\n",
    "\n",
    "# # Our \"permanent memory\" of visual features\n",
    "# tracked_identities = {}\n",
    "# next_permanent_id = 0\n",
    "\n",
    "# # --- NEW: A map to link temporary tracker IDs to our permanent Re-IDs ---\n",
    "# tracker_to_reid = {}\n",
    "\n",
    "# similarity_threshold = 0.90 # Lowered slightly for more robust matching\n",
    "# ptime = 0\n",
    "\n",
    "# # Use YOLO's tracker to get stable temporary IDs\n",
    "# for result in model.track(source=\"palace.mp4\", stream=True, tracker=\"bytetrack.yaml\",classes=[0]):\n",
    "#     frame = result.orig_img\n",
    "    \n",
    "#     if result.boxes.id is not None:\n",
    "#         boxes = result.boxes.xyxy.cpu().numpy()\n",
    "#         tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "#         # --- MODIFIED LOGIC: Iterate over boxes AND their temporary tracker IDs ---\n",
    "#         for box, tracker_id in zip(boxes, tracker_ids):\n",
    "#             final_id = -1\n",
    "            \n",
    "#             # --- Check if we've already assigned a permanent ID to this track ---\n",
    "#             if tracker_id in tracker_to_reid:\n",
    "#                 final_id = tracker_to_reid[tracker_id]\n",
    "#             else:\n",
    "#                 # --- This is a NEW track, so we perform Re-ID ---\n",
    "#                 x1, y1, x2, y2 = map(int, box)\n",
    "#                 crop = frame[y1:y2, x1:x2]\n",
    "#                 if crop.size == 0: continue\n",
    "#                 tensor_crop = transform(crop).unsqueeze(0).to(reid_predictor.MODEL.DEVICE)\n",
    "#                 # Extract visual features\n",
    "#                 features = rmodel(tensor_crop)\n",
    "#                 features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "#                 # Extract features\n",
    "#                 # features = reid_predictor([crop])[0]\n",
    "                \n",
    "#                 # --- Matching Logic ---\n",
    "#                 best_match_id = -1\n",
    "#                 max_similarity = -1\n",
    "\n",
    "#                 for pid, stored_feature in tracked_identities.items():\n",
    "#                     similarity = torch.nn.functional.cosine_similarity(features, stored_feature).item()\n",
    "#                     if similarity > max_similarity:\n",
    "#                         max_similarity = similarity\n",
    "#                         best_match_id = pid\n",
    "                \n",
    "#                 if max_similarity > similarity_threshold:\n",
    "#                     # It's a match with a previously lost person\n",
    "#                     final_id = best_match_id\n",
    "#                 else:\n",
    "#                     # It's a brand-new person\n",
    "#                     final_id = next_permanent_id\n",
    "#                     tracked_identities[final_id] = features\n",
    "#                     next_permanent_id += 1\n",
    "                \n",
    "#                 # --- CRUCIAL: Link the temporary ID to our new/matched permanent ID ---\n",
    "#                 tracker_to_reid[tracker_id] = final_id\n",
    "\n",
    "#             # --- Drawing Logic (now uses the stable final_id) ---\n",
    "#             print(len(tracked_identities),tracked_identities)\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "#             label = f\"Person ID: {final_id}\"\n",
    "#             cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#             cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # FPS Calculation\n",
    "#     ctime = time.time()\n",
    "#     if ptime != 0:\n",
    "#         fps = 1 / (ctime - ptime)\n",
    "#         cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "#     ptime = ctime\n",
    "\n",
    "#     cv.imshow(\"Manual ReID Tracking\", frame)\n",
    "#     if cv.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6378efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- IMPORTS ---\n",
    "# import cv2 as cv\n",
    "# import time\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from ultralytics import YOLO\n",
    "# import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# # Import what's needed from FastReID\n",
    "# from fastreid.config import get_cfg\n",
    "# from fastreid.modeling import build_model\n",
    "# from fastreid.utils.checkpoint import Checkpointer\n",
    "\n",
    "\n",
    "# # --- STEP 1: DEFINE THE IMAGE TRANSFORM ---\n",
    "# # This was missing. It's essential to resize and normalize the images\n",
    "# # exactly as the model expects.\n",
    "# # <-- ADD THIS ENTIRE BLOCK\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((256, 128)),  # Standard input size for Re-ID models\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "# def setup_reid_predictor():\n",
    "#     \"\"\"Sets up the configuration for the FastReID model.\"\"\"\n",
    "#     cfg = get_cfg()\n",
    "#     cfg.merge_from_file(\"C:/CCTV/fast-reid/configs/Market1501/bagtricks_R50.yml\")\n",
    "#     # This path must be correct.\n",
    "#     cfg.MODEL.WEIGHTS = \"C:/CCTV/market_bot_R50.pth\"\n",
    "#     cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     cfg.freeze()\n",
    "#     return cfg\n",
    "\n",
    "# # --- INITIALIZE MODELS ---\n",
    "# print(\"Initializing models...\")\n",
    "# # Object detection model\n",
    "# model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# # Re-identification model\n",
    "# reid_cfg = setup_reid_predictor()\n",
    "# rmodel = build_model(reid_cfg)\n",
    "# Checkpointer(rmodel).load(reid_cfg.MODEL.WEIGHTS)\n",
    "# rmodel.to(reid_cfg.MODEL.DEVICE)\n",
    "# rmodel.eval()\n",
    "# print(\"Models initialized successfully.\")\n",
    "\n",
    "# # --- TRACKING DATA STRUCTURES ---\n",
    "# # Our \"permanent memory\" of visual features for each person\n",
    "# tracked_identities = {}\n",
    "# next_permanent_id = 0\n",
    "\n",
    "# # A map to link temporary tracker IDs from YOLO to our permanent Re-IDs\n",
    "# tracker_to_reid = {}\n",
    "\n",
    "# # This threshold will now be meaningful. You might need to adjust it.\n",
    "# similarity_threshold = 0.50\n",
    "# ptime = 0\n",
    "\n",
    "# # --- MAIN VIDEO PROCESSING LOOP ---\n",
    "# # Use source=0 for webcam\n",
    "# for result in model.track(source=0, stream=True, tracker=\"bytetrack.yaml\", classes=[0]):\n",
    "#     frame = result.orig_img\n",
    "\n",
    "#     if result.boxes.id is None:\n",
    "#         # If no persons are tracked in the frame, just show it\n",
    "#         cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "#         if cv.waitKey(1) == ord('q'):\n",
    "#             break\n",
    "#         continue\n",
    "\n",
    "#     boxes = result.boxes.xyxy.cpu().numpy()\n",
    "#     tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "#     # --- Iterate over each tracked person ---\n",
    "#     for box, tracker_id in zip(boxes, tracker_ids):\n",
    "#         final_id = -1\n",
    "        \n",
    "#         # Check if we've already assigned a permanent ID to this track\n",
    "#         if tracker_id in tracker_to_reid:\n",
    "#             final_id = tracker_to_reid[tracker_id]\n",
    "#         else:\n",
    "#             # --- This is a NEW track, so we perform Re-ID ---\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "#             crop = frame[y1:y2, x1:x2]\n",
    "#             if crop.size == 0: continue\n",
    "\n",
    "#             # 1. Prepare the image using the defined transform\n",
    "#             tensor_crop = transform(crop).unsqueeze(0).to(reid_cfg.MODEL.DEVICE)\n",
    "#             F.to_pil_image(tensor_crop.squeeze(0).cpu()).save(\"check_input.jpg\")\n",
    "#             # 2. Extract features\n",
    "#             features = rmodel(tensor_crop)\n",
    "            \n",
    "#             # --- STEP 2: NORMALIZE THE FEATURES ---\n",
    "#             # This was missing. It makes the comparison fair and meaningful.\n",
    "#             # <-- ADD THIS LINE\n",
    "#             features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "            \n",
    "#             # --- Matching Logic ---\n",
    "#             best_match_id = -1\n",
    "#             max_similarity = -1\n",
    "\n",
    "#             for pid, stored_feature in tracked_identities.items():\n",
    "#                 # Compare the new person's features with all known persons\n",
    "#                 similarity = torch.nn.functional.cosine_similarity(features, stored_feature).item()\n",
    "#                 if similarity > max_similarity:\n",
    "#                     max_similarity = similarity\n",
    "#                     best_match_id = pid\n",
    "            \n",
    "#             if max_similarity > similarity_threshold:\n",
    "#                 # It's a match! Re-assign the permanent ID of the matched person.\n",
    "#                 final_id = best_match_id\n",
    "#             else:\n",
    "#                 # It's a brand-new person. Assign a new permanent ID.\n",
    "#                 final_id = next_permanent_id\n",
    "#                 tracked_identities[final_id] = features  # Store the NORMALIZED features\n",
    "#                 next_permanent_id += 1\n",
    "            \n",
    "#             # Link the temporary YOLO tracker ID to our permanent Re-ID\n",
    "#             tracker_to_reid[tracker_id] = final_id\n",
    "#             print(len(tracked_identities),tracked_identities)\n",
    "#         # --- Drawing Logic ---\n",
    "#         x1, y1, x2, y2 = map(int, box)\n",
    "#         label = f\"ID: {final_id}\"\n",
    "#         cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # --- FPS Calculation ---\n",
    "#     ctime = time.time()\n",
    "#     if ptime != 0:\n",
    "#         fps = 1 / (ctime - ptime)\n",
    "#         cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "#     ptime = ctime\n",
    "\n",
    "#     cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "#     if cv.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1de92b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- IMPORTS ---\n",
    "# import cv2 as cv\n",
    "# from torchvision import transforms\n",
    "# import torchvision.transforms.functional as F\n",
    "# import time\n",
    "# import torch\n",
    "# # We no longer need torchvision.transforms!\n",
    "# # from torchvision import transforms\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Import what's needed from FastReID\n",
    "# from fastreid.config import get_cfg\n",
    "# # Use the high-level predictor instead of manual components\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((256, 128)),  # Standard input size for Re-ID models\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "# def setup_reid_predictor():\n",
    "#     \"\"\"Sets up the configuration for the FastReID model.\"\"\"\n",
    "#     cfg = get_cfg()\n",
    "#     cfg.merge_from_file(\"C:/CCTV/fast-reid/configs/Market1501/bagtricks_R50.yml\")\n",
    "#     # This path must be correct.\n",
    "#     cfg.MODEL.WEIGHTS = \"C:/CCTV/market_bot_R50.pth\"\n",
    "#     cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     cfg.freeze()\n",
    "#     return cfg\n",
    "\n",
    "# # --- INITIALIZE MODELS ---\n",
    "# print(\"Initializing models...\")\n",
    "# # Object detection model\n",
    "# model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# # --- NEW: INITIALIZE THE RE-ID PREDICTOR ---\n",
    "# # This one line replaces the manual build_model, Checkpointer, and rmodel.eval()\n",
    "# reid_cfg = setup_reid_predictor()\n",
    "# predictor = DefaultPredictor(reid_cfg)\n",
    "# print(\"Models initialized successfully.\")\n",
    "\n",
    "# # --- TRACKING DATA STRUCTURES ---\n",
    "# tracked_identities = {}\n",
    "# next_permanent_id = 0\n",
    "# tracker_to_reid = {}\n",
    "\n",
    "# # Start with a reasonable threshold\n",
    "# similarity_threshold = 0.95\n",
    "# ptime = 0\n",
    "\n",
    "# # --- MAIN VIDEO PROCESSING LOOP ---\n",
    "# for result in model.track(source=\"palace.mp4\", stream=True, tracker=\"bytetrack.yaml\", classes=[0]):\n",
    "#     frame = result.orig_img\n",
    "\n",
    "#     if result.boxes.id is None:\n",
    "#         cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "#         if cv.waitKey(1) == ord('q'): break\n",
    "#         continue\n",
    "\n",
    "#     boxes = result.boxes.xyxy.cpu().numpy()\n",
    "#     tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "#     for box, tracker_id in zip(boxes, tracker_ids):\n",
    "#         final_id = -1\n",
    "        \n",
    "#         if tracker_id in tracker_to_reid:\n",
    "#             final_id = tracker_to_reid[tracker_id]\n",
    "#         else:\n",
    "#             # --- This is the new, corrected block ---\n",
    "#             # This is a NEW track, so we perform Re-ID\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "#             crop = frame[y1:y2, x1:x2]\n",
    "#             if crop.size == 0: continue\n",
    "\n",
    "#             # 1. Convert NumPy crop to a PyTorch tensor with the correct shape (B, C, H, W)\n",
    "#            # Apply the correct transform (resize + normalize)\n",
    "#             processed_crop = transform(crop).unsqueeze(0).to(reid_cfg.MODEL.DEVICE)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 predictor.model.eval()\n",
    "#                 features = predictor(processed_crop)\n",
    "#                 # features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "            \n",
    "#             # --- Matching Logic ---\n",
    "#             best_match_id = -1\n",
    "#             max_similarity = -1\n",
    "\n",
    "#             for pid, stored_feature in tracked_identities.items():\n",
    "#                 similarity = torch.nn.functional.cosine_similarity(features, stored_feature).item()\n",
    "#                 if similarity > max_similarity:\n",
    "#                     max_similarity = similarity\n",
    "#                     best_match_id = pid\n",
    "            \n",
    "#             if max_similarity > similarity_threshold:\n",
    "#                 final_id = best_match_id\n",
    "#                 # Optional: Update the stored feature with the new one to adapt to appearance changes\n",
    "#                 tracked_identities[final_id] = features\n",
    "#             else:\n",
    "#                 final_id = next_permanent_id\n",
    "#                 tracked_identities[final_id] = features\n",
    "#                 next_permanent_id += 1\n",
    "#             print(len(tracked_identities),tracked_identities)\n",
    "#             tracker_to_reid[tracker_id] = final_id\n",
    "            \n",
    "#         # --- Drawing Logic ---\n",
    "#         x1, y1, x2, y2 = map(int, box)\n",
    "#         label = f\"ID: {final_id}\"\n",
    "#         cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # --- FPS Calculation ---\n",
    "#     ctime = time.time()\n",
    "#     if ptime != 0:\n",
    "#         fps = 1 / (ctime - ptime)\n",
    "#         cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "#     ptime = ctime\n",
    "\n",
    "#     cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "#     if cv.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a660aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # It is assumed that this script is run from a directory that contains\n",
    "# # the cloned 'fast-reid' repository.\n",
    "# # Add the fast-reid repository to the Python path.\n",
    "# import sys\n",
    "# sys.path.append('fast-reid')\n",
    "\n",
    "# from fastreid.config import get_cfg\n",
    "# from fastreid.modeling import build_model\n",
    "# from fastreid.utils.checkpoint import Checkpointer\n",
    "# import fastreid.modeling.heads \n",
    "# import fastreid.modeling.backbones\n",
    "# def setup_cfg(config_file, weights_file):\n",
    "#     \"\"\"\n",
    "#     Creates a FastReID config object and merges it with the specified\n",
    "#     configuration file and weights.\n",
    "#     \"\"\"\n",
    "#     # 1. Get a copy of the default configuration\n",
    "#     cfg = get_cfg()\n",
    "\n",
    "#     # 2. Add project-specific configurations if any\n",
    "#     # (Not needed for this baseline model)\n",
    "\n",
    "#     # 3. Merge from the specified YAML configuration file\n",
    "#     # This file contains the model architecture and dataset specifics.\n",
    "#     cfg.merge_from_file(config_file)\n",
    "\n",
    "#     # 4. Set the path to the pre-trained model weights.\n",
    "#     # This is a critical step for loading the desired model state.\n",
    "#     cfg.MODEL.WEIGHTS = weights_file\n",
    "\n",
    "#     # 5. Set the device to use for inference.\n",
    "#     # Use 'cuda' for GPU or 'cpu' for CPU.\n",
    "#     cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     cfg.MODEL.HEADS.NUM_CLASSES = 751\n",
    "#     # 6. Freeze the configuration.\n",
    "#     # This makes the config object immutable and prevents accidental changes.\n",
    "#     cfg.freeze()\n",
    "    \n",
    "#     return cfg\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main function to load the model and print its architecture.\n",
    "#     \"\"\"\n",
    "#     # --- Configuration ---\n",
    "#     # Define paths relative to the project structure.\n",
    "#     project_root = Path('.')\n",
    "#     fastreid_repo_path = project_root / 'fast-reid'\n",
    "    \n",
    "#     # Path to the configuration YAML file.\n",
    "#     config_file_path = fastreid_repo_path / 'configs/Market1501/bagtricks_R50.yml'\n",
    "    \n",
    "#     # Path to the pre-trained weights file.\n",
    "#     weights_file_path = project_root / 'models/market_bot_R50.pth'\n",
    "\n",
    "#     # Verify that the necessary files exist.\n",
    "#     if not config_file_path.is_file():\n",
    "#         print(f\"Error: Config file not found at {config_file_path}\")\n",
    "#         return\n",
    "#     if not weights_file_path.is_file():\n",
    "#         print(f\"Error: Weights file not found at {weights_file_path}\")\n",
    "#         print(\"Please download 'market_bot_R50.pth' and place it in the 'models/' directory.\")\n",
    "#         return\n",
    "\n",
    "#     # --- Model Loading ---\n",
    "#     # 1. Setup the configuration object.\n",
    "#     cfg = setup_cfg(str(config_file_path), str(weights_file_path))\n",
    "#     print(f\"Configuration loaded successfully. Using device: {cfg.MODEL.DEVICE}\")\n",
    "\n",
    "#     # 2. Build the model.\n",
    "#     # The `build_model` function reads the configuration and constructs the\n",
    "#     # corresponding model architecture. It also automatically loads the weights\n",
    "#     # specified in `cfg.MODEL.WEIGHTS`.\n",
    "#     model = build_model(cfg)\n",
    "#     print(\"Model built successfully.\")\n",
    "\n",
    "#     # The Checkpointer is used internally by `build_model` but can also be\n",
    "#     # used explicitly if needed. For this use case, `build_model` handles it.\n",
    "#     # Checkpointer(model).load(cfg.MODEL.WEIGHTS) # This is done inside build_model\n",
    "\n",
    "#     # 3. Set the model to evaluation mode.\n",
    "#     # This is crucial for consistent results during inference as it disables\n",
    "#     # layers like Dropout and uses running statistics for Batch Normalization.\n",
    "#     model.eval()\n",
    "#     print(\"Model set to evaluation mode.\")\n",
    "\n",
    "#     # --- Robust Weight Verification ---\n",
    "#    # --- Manual Weight Correction and Loading ---\n",
    "#     print(\"\\n--- Manually Correcting and Loading Weights ---\")\n",
    "    \n",
    "#     device = torch.device(cfg.MODEL.DEVICE)\n",
    "#     weights_from_file = torch.load(weights_file_path, map_location=device)\n",
    "    \n",
    "#     # Check if weights are nested under a 'model' key\n",
    "#     if 'model' in weights_from_file:\n",
    "#         weights_from_file = weights_from_file['model']\n",
    "\n",
    "#     # Create a new dictionary to hold the corrected weights\n",
    "#     from collections import OrderedDict\n",
    "#     corrected_weights = OrderedDict()\n",
    "    \n",
    "#     # Iterate through the keys from the file and rename them\n",
    "#     for k, v in weights_from_file.items():\n",
    "#         # --- THIS IS THE FIX ---\n",
    "#         if k in ['pixel_mean', 'pixel_std']:\n",
    "#             continue\n",
    "#         if k.startswith('module.'):\n",
    "#              # Remove 'module.' prefix if it exists\n",
    "#             new_key = k[7:]\n",
    "#         else:\n",
    "#             new_key = k\n",
    "        \n",
    "#         # Rename the head layers\n",
    "#         if new_key == 'heads.classifier.weight':\n",
    "#             new_key = 'heads.weight'\n",
    "#         elif 'heads.bnneck.' in new_key:\n",
    "#             new_key = new_key.replace('heads.bnneck.', 'heads.bottleneck.0.')\n",
    "        \n",
    "#         corrected_weights[new_key] = v\n",
    "#         # --------------------\n",
    "\n",
    "#     # Load the corrected weights into the model.\n",
    "#     # We use strict=False to ignore keys that are in the file but not needed by the model\n",
    "#     # (like pixel_mean, pixel_std).\n",
    "#     incompatible = model.load_state_dict(corrected_weights, strict=False)\n",
    "    \n",
    "#     if not incompatible.missing_keys and not incompatible.unexpected_keys:\n",
    "#         print(\"✅ Success! All model weights were loaded correctly after manual correction.\")\n",
    "#     else:\n",
    "#         print(\"❌ Error! Still some mismatches after correction.\")\n",
    "#         print(f\"Missing keys in model: {incompatible.missing_keys}\")\n",
    "#         print(f\"Unexpected keys in file: {incompatible.unexpected_keys}\")\n",
    "\n",
    "#     print(\"--------------------------------\\n\")\n",
    "\n",
    "#     # --- Verification ---\n",
    "#     # ... (the rest of the script, like printing the model architecture) ...\n",
    "\n",
    "#     # --- Verification ---\n",
    "#     # Print the model architecture to verify it has been loaded correctly.\n",
    "#     # You should see the ResNet-50 blocks, the pooling layer, and the BNNeck.\n",
    "#     print(\"\\n--- Model Architecture ---\")\n",
    "#     print(model)\n",
    "#     print(\"--------------------------\\n\")\n",
    "    \n",
    "#     print(\"FastReID model is ready for feature extraction.\")\n",
    "    \n",
    "#     # The 'model' object is now ready to be used for inference.\n",
    "#     return model\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     loaded_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3460e6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4945070147514343"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(loaded_model(transform(cv.imread(\"C:/CCTV/images/1.jpg\"))), loaded_model(transform(cv.imread(\"C:/CCTV/images/2.jpg\")))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38f6e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2 as cv\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image \n",
    "# from torchvision import transforms\n",
    "# from fastreid.modeling import build_model\n",
    "# # Define transformation to tensor and normalization (based on your ReID model's requirements)\n",
    "# # transform = transforms.Compose([\n",
    "# #     transforms.ToPILImage(),\n",
    "# #     transforms.Resize((256, 128)),  # standard size for ReID models like FastReID\n",
    "# #     transforms.ToTensor(),\n",
    "# #     # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "# # ])\n",
    "\n",
    "# import cv2 as cv\n",
    "# import time\n",
    "# import torch\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "\n",
    "# tracked_identities = {}\n",
    "# next_permanent_id = 0\n",
    "# similarity_threshold = 0.7\n",
    "\n",
    "# ptime = 0\n",
    "\n",
    "# # Use a basic tracker here; we will handle the ReID part ourselves\n",
    "# for result in model.track(source=\"palace.mp4\", stream=True, tracker=\"bytetrack.yaml\", classes=[0]):\n",
    "#     frame = result.orig_img\n",
    "    \n",
    "#     if result.boxes.id is not None:\n",
    "#         boxes = result.boxes.xyxy.cpu().numpy()\n",
    "\n",
    "#         for box in boxes:\n",
    "#             x1, y1, x2, y2 = map(int, box)\n",
    "            \n",
    "#             # Crop the detected person\n",
    "#             crop = frame[y1:y2, x1:x2]\n",
    "#             if crop.size == 0: continue\n",
    "#             tensor_crop = transform(crop) \n",
    "#             # Extract visual features\n",
    "            \n",
    "#             features = loaded_model(tensor_crop)\n",
    "            \n",
    "#             # --- Matching Logic ---\n",
    "#             best_match_id = -1\n",
    "#             max_similarity = -1\n",
    "\n",
    "#             # Compare the new features to all known identities\n",
    "#             for pid, stored_feature in tracked_identities.items():\n",
    "#                 similarity = torch.nn.functional.cosine_similarity(features, stored_feature).item()\n",
    "#                 if similarity > max_similarity:\n",
    "#                     max_similarity = similarity\n",
    "#                     best_match_id = pid\n",
    "            \n",
    "#             final_id = -1\n",
    "#             if max_similarity > similarity_threshold:\n",
    "#                 final_id = best_match_id # It's a match!\n",
    "#                 tracked_identities[final_id] = features # Update features\n",
    "#             else:\n",
    "#                 final_id = next_permanent_id # It's a new person\n",
    "                \n",
    "#                 tracked_identities[final_id] = features\n",
    "#                 next_permanent_id += 1\n",
    "#             print(len(tracked_identities),tracked_identities)\n",
    "\n",
    "#             # --- Drawing Logic ---\n",
    "#             label = f\"Person ID: {final_id}\"\n",
    "#             cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#             cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "#     # FPS Calculation\n",
    "#     ctime = time.time()\n",
    "#     if ptime != 0:\n",
    "#         fps = 1 / (ctime - ptime)\n",
    "#         cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "#     ptime = ctime\n",
    "\n",
    "#     cv.imshow(\"Manual ReID Tracking\", frame)\n",
    "#     if cv.waitKey(1) == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0fef08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "video 1/1 (frame 1/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 36.3ms\n",
      "1 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0')}\n",
      "2 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0')}\n",
      "3 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0')}\n",
      "4 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0')}\n",
      "5 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0')}\n",
      "6 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0')}\n",
      "7 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0')}\n",
      "8 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9548, -1.0694,  1.1358,  ..., -0.1005,  0.4375, -0.2786]], device='cuda:0')}\n",
      "9 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9548, -1.0694,  1.1358,  ..., -0.1005,  0.4375, -0.2786]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0')}\n",
      "10 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9548, -1.0694,  1.1358,  ..., -0.1005,  0.4375, -0.2786]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0')}\n",
      "video 1/1 (frame 2/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 3/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.0ms\n",
      "video 1/1 (frame 4/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 5/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 6/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 7/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 8/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 9/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 10/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "11 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9548, -1.0694,  1.1358,  ..., -0.1005,  0.4375, -0.2786]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0')}\n",
      "12 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9548, -1.0694,  1.1358,  ..., -0.1005,  0.4375, -0.2786]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0')}\n",
      "video 1/1 (frame 11/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 12/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 13/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 14/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 15/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 16/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 17/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 18/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 19/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 20/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.5ms\n",
      "video 1/1 (frame 21/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.8ms\n",
      "video 1/1 (frame 22/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 23/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.2ms\n",
      "video 1/1 (frame 24/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 12.3ms\n",
      "video 1/1 (frame 25/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 26/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.8ms\n",
      "video 1/1 (frame 27/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.8ms\n",
      "video 1/1 (frame 28/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 29/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 30/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 11.8ms\n",
      "video 1/1 (frame 31/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "13 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9548, -1.0694,  1.1358,  ..., -0.1005,  0.4375, -0.2786]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0')}\n",
      "video 1/1 (frame 32/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 33/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 34/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 35/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 36/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 37/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 38/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 39/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 40/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 41/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 42/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 43/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 44/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 45/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 46/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 47/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 48/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 49/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 50/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 51/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 52/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 53/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.9ms\n",
      "video 1/1 (frame 54/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 55/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 56/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 57/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.0ms\n",
      "video 1/1 (frame 58/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 59/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 60/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "13 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0')}\n",
      "video 1/1 (frame 61/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 62/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 63/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.0ms\n",
      "14 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0')}\n",
      "video 1/1 (frame 64/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.6ms\n",
      "video 1/1 (frame 65/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.8ms\n",
      "video 1/1 (frame 66/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.2ms\n",
      "video 1/1 (frame 67/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.8ms\n",
      "video 1/1 (frame 68/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 69/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.8ms\n",
      "video 1/1 (frame 70/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 71/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 72/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 10.7ms\n",
      "video 1/1 (frame 73/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.2ms\n",
      "video 1/1 (frame 74/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.3ms\n",
      "15 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0')}\n",
      "video 1/1 (frame 75/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 76/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.1ms\n",
      "video 1/1 (frame 77/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.1ms\n",
      "video 1/1 (frame 78/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.1ms\n",
      "16 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0')}\n",
      "video 1/1 (frame 79/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.5ms\n",
      "video 1/1 (frame 80/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.2ms\n",
      "video 1/1 (frame 81/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.3ms\n",
      "video 1/1 (frame 82/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 10.6ms\n",
      "video 1/1 (frame 83/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.4ms\n",
      "video 1/1 (frame 84/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.3ms\n",
      "video 1/1 (frame 85/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.6ms\n",
      "video 1/1 (frame 86/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 87/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 88/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 89/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 90/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 10.4ms\n",
      "video 1/1 (frame 91/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.1ms\n",
      "video 1/1 (frame 92/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.8ms\n",
      "video 1/1 (frame 93/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.2ms\n",
      "video 1/1 (frame 94/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 95/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.1ms\n",
      "video 1/1 (frame 96/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.1ms\n",
      "video 1/1 (frame 97/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.4ms\n",
      "video 1/1 (frame 98/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.0ms\n",
      "video 1/1 (frame 99/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.6ms\n",
      "video 1/1 (frame 100/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.3ms\n",
      "video 1/1 (frame 101/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.1ms\n",
      "video 1/1 (frame 102/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.0ms\n",
      "video 1/1 (frame 103/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.0ms\n",
      "video 1/1 (frame 104/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.0ms\n",
      "video 1/1 (frame 105/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.4ms\n",
      "video 1/1 (frame 106/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.5ms\n",
      "video 1/1 (frame 107/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.4ms\n",
      "video 1/1 (frame 108/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.5ms\n",
      "video 1/1 (frame 109/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.6ms\n",
      "video 1/1 (frame 110/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.7ms\n",
      "video 1/1 (frame 111/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.3ms\n",
      "video 1/1 (frame 112/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.3ms\n",
      "video 1/1 (frame 113/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.4ms\n",
      "video 1/1 (frame 114/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.8ms\n",
      "video 1/1 (frame 115/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.6ms\n",
      "video 1/1 (frame 116/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.9ms\n",
      "video 1/1 (frame 117/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.4ms\n",
      "video 1/1 (frame 118/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.6ms\n",
      "video 1/1 (frame 119/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.7ms\n",
      "video 1/1 (frame 120/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.3ms\n",
      "video 1/1 (frame 121/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.7ms\n",
      "video 1/1 (frame 122/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.7ms\n",
      "video 1/1 (frame 123/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.4ms\n",
      "video 1/1 (frame 124/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.7ms\n",
      "video 1/1 (frame 125/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.1ms\n",
      "video 1/1 (frame 126/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.8ms\n",
      "video 1/1 (frame 127/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.4ms\n",
      "video 1/1 (frame 128/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 10.9ms\n",
      "video 1/1 (frame 129/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.5ms\n",
      "video 1/1 (frame 130/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.5ms\n",
      "video 1/1 (frame 131/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.5ms\n",
      "video 1/1 (frame 132/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 10.8ms\n",
      "video 1/1 (frame 133/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.4ms\n",
      "video 1/1 (frame 134/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 11.2ms\n",
      "video 1/1 (frame 135/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.6ms\n",
      "video 1/1 (frame 136/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.5ms\n",
      "video 1/1 (frame 137/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.4ms\n",
      "video 1/1 (frame 138/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.2ms\n",
      "video 1/1 (frame 139/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.5ms\n",
      "video 1/1 (frame 140/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.6ms\n",
      "video 1/1 (frame 141/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.5ms\n",
      "video 1/1 (frame 142/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.9ms\n",
      "video 1/1 (frame 143/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.4ms\n",
      "video 1/1 (frame 144/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.8ms\n",
      "video 1/1 (frame 145/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.5ms\n",
      "video 1/1 (frame 146/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.6ms\n",
      "video 1/1 (frame 147/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.5ms\n",
      "video 1/1 (frame 148/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.7ms\n",
      "video 1/1 (frame 149/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.8ms\n",
      "video 1/1 (frame 150/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.9ms\n",
      "video 1/1 (frame 151/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.9ms\n",
      "video 1/1 (frame 152/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 153/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.5ms\n",
      "video 1/1 (frame 154/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.5ms\n",
      "video 1/1 (frame 155/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.8ms\n",
      "video 1/1 (frame 156/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 157/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.4ms\n",
      "video 1/1 (frame 158/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.5ms\n",
      "video 1/1 (frame 159/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.5ms\n",
      "video 1/1 (frame 160/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.7ms\n",
      "video 1/1 (frame 161/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.7ms\n",
      "video 1/1 (frame 162/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.5ms\n",
      "video 1/1 (frame 163/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.5ms\n",
      "video 1/1 (frame 164/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.4ms\n",
      "video 1/1 (frame 165/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.6ms\n",
      "video 1/1 (frame 166/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 167/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.0ms\n",
      "video 1/1 (frame 168/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 169/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 11.5ms\n",
      "17 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0')}\n",
      "video 1/1 (frame 170/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.0ms\n",
      "video 1/1 (frame 171/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.5ms\n",
      "video 1/1 (frame 172/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.4ms\n",
      "video 1/1 (frame 173/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.4ms\n",
      "video 1/1 (frame 174/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 11.0ms\n",
      "video 1/1 (frame 175/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.6ms\n",
      "video 1/1 (frame 176/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.5ms\n",
      "video 1/1 (frame 177/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.5ms\n",
      "video 1/1 (frame 178/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.8ms\n",
      "video 1/1 (frame 179/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.2ms\n",
      "video 1/1 (frame 180/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 181/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.6ms\n",
      "video 1/1 (frame 182/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.4ms\n",
      "video 1/1 (frame 183/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.4ms\n",
      "video 1/1 (frame 184/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 185/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "18 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0'), 17: tensor([[-0.9597, -1.0306,  0.3697,  ..., -0.1384,  0.4667, -0.1961]], device='cuda:0')}\n",
      "video 1/1 (frame 186/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 187/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 188/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 189/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 190/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 191/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.4ms\n",
      "video 1/1 (frame 192/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 193/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.4ms\n",
      "video 1/1 (frame 194/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.4ms\n",
      "19 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0'), 17: tensor([[-0.9597, -1.0306,  0.3697,  ..., -0.1384,  0.4667, -0.1961]], device='cuda:0'), 18: tensor([[-0.7848, -0.5546,  1.8378,  ...,  0.0959, -0.1303, -0.2240]], device='cuda:0')}\n",
      "video 1/1 (frame 195/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 196/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.3ms\n",
      "video 1/1 (frame 197/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.5ms\n",
      "video 1/1 (frame 198/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.3ms\n",
      "video 1/1 (frame 199/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.9ms\n",
      "video 1/1 (frame 200/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 201/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 202/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 203/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 204/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 205/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 206/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 207/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 208/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 209/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 210/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 211/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 212/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 213/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 214/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 215/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 216/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 217/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 218/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 219/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 220/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "20 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0'), 17: tensor([[-0.9597, -1.0306,  0.3697,  ..., -0.1384,  0.4667, -0.1961]], device='cuda:0'), 18: tensor([[-0.7848, -0.5546,  1.8378,  ...,  0.0959, -0.1303, -0.2240]], device='cuda:0'), 19: tensor([[-0.3971, -1.0395,  0.2351,  ..., -0.2343,  0.2305, -0.4288]], device='cuda:0')}\n",
      "video 1/1 (frame 221/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.1ms\n",
      "video 1/1 (frame 222/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.1ms\n",
      "video 1/1 (frame 223/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.5ms\n",
      "21 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0'), 17: tensor([[-0.9597, -1.0306,  0.3697,  ..., -0.1384,  0.4667, -0.1961]], device='cuda:0'), 18: tensor([[-0.7848, -0.5546,  1.8378,  ...,  0.0959, -0.1303, -0.2240]], device='cuda:0'), 19: tensor([[-0.3971, -1.0395,  0.2351,  ..., -0.2343,  0.2305, -0.4288]], device='cuda:0'), 20: tensor([[-0.8303, -0.9780, -0.1786,  ..., -0.2128,  0.0484, -0.6432]], device='cuda:0')}\n",
      "video 1/1 (frame 224/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.0ms\n",
      "video 1/1 (frame 225/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 226/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 12.9ms\n",
      "video 1/1 (frame 227/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.4ms\n",
      "video 1/1 (frame 228/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 229/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.3ms\n",
      "video 1/1 (frame 230/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 231/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.6ms\n",
      "video 1/1 (frame 232/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 233/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 234/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.2ms\n",
      "22 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0'), 17: tensor([[-0.9597, -1.0306,  0.3697,  ..., -0.1384,  0.4667, -0.1961]], device='cuda:0'), 18: tensor([[-0.7848, -0.5546,  1.8378,  ...,  0.0959, -0.1303, -0.2240]], device='cuda:0'), 19: tensor([[-0.3971, -1.0395,  0.2351,  ..., -0.2343,  0.2305, -0.4288]], device='cuda:0'), 20: tensor([[-0.8303, -0.9780, -0.1786,  ..., -0.2128,  0.0484, -0.6432]], device='cuda:0'), 21: tensor([[-0.7985, -0.9458,  0.2385,  ..., -0.0928,  0.0220, -0.0594]], device='cuda:0')}\n",
      "video 1/1 (frame 235/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.2ms\n",
      "22 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0'), 17: tensor([[-0.9597, -1.0306,  0.3697,  ..., -0.1384,  0.4667, -0.1961]], device='cuda:0'), 18: tensor([[-0.6446, -0.6606,  1.4943,  ...,  0.1067, -0.1697, -0.0386]], device='cuda:0'), 19: tensor([[-0.3971, -1.0395,  0.2351,  ..., -0.2343,  0.2305, -0.4288]], device='cuda:0'), 20: tensor([[-0.8303, -0.9780, -0.1786,  ..., -0.2128,  0.0484, -0.6432]], device='cuda:0'), 21: tensor([[-0.7985, -0.9458,  0.2385,  ..., -0.0928,  0.0220, -0.0594]], device='cuda:0')}\n",
      "video 1/1 (frame 236/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.6ms\n",
      "video 1/1 (frame 237/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.4ms\n",
      "video 1/1 (frame 238/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.2ms\n",
      "video 1/1 (frame 239/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.4ms\n",
      "video 1/1 (frame 240/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.3ms\n",
      "video 1/1 (frame 241/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.3ms\n",
      "video 1/1 (frame 242/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.3ms\n",
      "video 1/1 (frame 243/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.2ms\n",
      "video 1/1 (frame 244/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.6ms\n",
      "video 1/1 (frame 245/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 246/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.3ms\n",
      "video 1/1 (frame 247/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 248/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 249/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 250/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 251/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 252/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 253/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.6ms\n",
      "video 1/1 (frame 254/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 255/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 256/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 257/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 258/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 259/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.6ms\n",
      "video 1/1 (frame 260/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 261/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 262/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 263/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 264/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 265/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 266/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 267/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 268/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 269/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.1ms\n",
      "video 1/1 (frame 270/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.4ms\n",
      "video 1/1 (frame 271/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 272/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.7ms\n",
      "video 1/1 (frame 273/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 274/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.3ms\n",
      "video 1/1 (frame 275/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 276/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 277/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.9ms\n",
      "video 1/1 (frame 278/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.1ms\n",
      "video 1/1 (frame 279/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 280/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.6ms\n",
      "video 1/1 (frame 281/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.6ms\n",
      "video 1/1 (frame 282/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 283/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 284/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 285/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 286/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.6ms\n",
      "video 1/1 (frame 287/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 11.2ms\n",
      "video 1/1 (frame 288/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.9ms\n",
      "video 1/1 (frame 289/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 290/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 291/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.8ms\n",
      "video 1/1 (frame 292/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 293/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 294/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 295/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.5ms\n",
      "video 1/1 (frame 296/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 11.4ms\n",
      "video 1/1 (frame 297/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 298/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.8ms\n",
      "video 1/1 (frame 299/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.0ms\n",
      "video 1/1 (frame 300/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.6ms\n",
      "video 1/1 (frame 301/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 302/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.7ms\n",
      "video 1/1 (frame 303/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.7ms\n",
      "video 1/1 (frame 304/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.6ms\n",
      "video 1/1 (frame 305/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.7ms\n",
      "video 1/1 (frame 306/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.6ms\n",
      "video 1/1 (frame 307/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.3ms\n",
      "video 1/1 (frame 308/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.0ms\n",
      "video 1/1 (frame 309/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.6ms\n",
      "video 1/1 (frame 310/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.0ms\n",
      "video 1/1 (frame 311/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.2ms\n",
      "video 1/1 (frame 312/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.3ms\n",
      "video 1/1 (frame 313/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.3ms\n",
      "video 1/1 (frame 314/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.3ms\n",
      "video 1/1 (frame 315/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.0ms\n",
      "video 1/1 (frame 316/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 317/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.0ms\n",
      "video 1/1 (frame 318/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.7ms\n",
      "video 1/1 (frame 319/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.1ms\n",
      "video 1/1 (frame 320/329) c:\\CCTV\\palace.mp4: 384x640 8 persons, 8.0ms\n",
      "video 1/1 (frame 321/329) c:\\CCTV\\palace.mp4: 384x640 8 persons, 8.3ms\n",
      "video 1/1 (frame 322/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.1ms\n",
      "video 1/1 (frame 323/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "23 {0: tensor([[-0.5008,  0.0561,  0.3553,  ..., -0.1541,  0.0792, -0.1903]], device='cuda:0'), 1: tensor([[-1.0304, -1.0433,  0.2717,  ..., -0.1313,  1.2633, -0.0835]], device='cuda:0'), 2: tensor([[-0.2401, -0.9490,  0.0121,  ..., -0.2927,  0.3419,  0.3385]], device='cuda:0'), 3: tensor([[-0.6003, -0.2303,  0.2656,  ..., -0.0382,  0.1661, -0.7054]], device='cuda:0'), 4: tensor([[-0.2699, -1.0469, -0.0940,  ..., -0.3215, -0.1849, -0.4895]], device='cuda:0'), 5: tensor([[-0.5781, -0.7480,  0.0243,  ..., -0.1460,  0.2734,  0.1095]], device='cuda:0'), 6: tensor([[-0.6636, -0.9552,  0.1908,  ..., -0.1671, -0.0703,  0.7445]], device='cuda:0'), 7: tensor([[-0.9600, -0.8961,  0.9893,  ..., -0.1280,  0.2728,  0.0023]], device='cuda:0'), 8: tensor([[-0.4899, -0.8104,  0.0509,  ..., -0.0801, -0.2897, -0.2723]], device='cuda:0'), 9: tensor([[-0.7437, -0.9623, -0.4142,  ..., -0.1702,  0.0465, -0.4876]], device='cuda:0'), 10: tensor([[-1.0903, -1.0394,  0.0404,  ..., -0.1124, -0.1489, -0.4427]], device='cuda:0'), 11: tensor([[-0.5394, -0.7805,  0.5720,  ..., -0.1545,  1.0152,  0.2757]], device='cuda:0'), 12: tensor([[-0.4870, -1.0022, -0.1748,  ...,  0.0331,  0.7849, -0.4622]], device='cuda:0'), 13: tensor([[-0.4028, -1.0445,  0.0328,  ..., -0.2486, -0.5610, -0.3293]], device='cuda:0'), 14: tensor([[-0.4797, -0.8301, -0.0744,  ..., -0.1627, -0.1499,  0.2250]], device='cuda:0'), 15: tensor([[-0.2466, -0.7720, -0.2742,  ..., -0.1645, -0.3141, -0.1531]], device='cuda:0'), 16: tensor([[-0.5886, -1.0154,  0.1663,  ..., -0.1998,  0.8568, -0.1092]], device='cuda:0'), 17: tensor([[-0.9597, -1.0306,  0.3697,  ..., -0.1384,  0.4667, -0.1961]], device='cuda:0'), 18: tensor([[-0.6446, -0.6606,  1.4943,  ...,  0.1067, -0.1697, -0.0386]], device='cuda:0'), 19: tensor([[-0.3971, -1.0395,  0.2351,  ..., -0.2343,  0.2305, -0.4288]], device='cuda:0'), 20: tensor([[-0.8303, -0.9780, -0.1786,  ..., -0.2128,  0.0484, -0.6432]], device='cuda:0'), 21: tensor([[-0.7985, -0.9458,  0.2385,  ..., -0.0928,  0.0220, -0.0594]], device='cuda:0'), 22: tensor([[-0.6163, -1.0834,  0.1746,  ..., -0.1725,  0.6047, -0.8747]], device='cuda:0')}\n",
      "video 1/1 (frame 324/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 325/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 9.5ms\n",
      "video 1/1 (frame 326/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.2ms\n",
      "video 1/1 (frame 327/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.4ms\n",
      "video 1/1 (frame 328/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.0ms\n",
      "video 1/1 (frame 329/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.0ms\n",
      "Speed: 1.1ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import cv2 as cv\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import time\n",
    "import torch\n",
    "# We no longer need torchvision.transforms!\n",
    "# from torchvision import transforms\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Import what's needed from FastReID\n",
    "from fastreid.config import get_cfg\n",
    "# # Use the high-level predictor instead of manual components\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((256, 128)),  # Standard input size for Re-ID models\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "# --- TRACKING DATA STRUCTURES ---\n",
    "tracked_identities = {}\n",
    "next_permanent_id = 0\n",
    "tracker_to_reid = {}\n",
    "\n",
    "# Start with a reasonable threshold\n",
    "similarity_threshold = 0.9\n",
    "ptime = 0\n",
    "\n",
    "# --- MAIN VIDEO PROCESSING LOOP ---\n",
    "for result in model3.track(source=\"palace.mp4\", stream=True, tracker=\"bytetrack.yaml\",classes=[0]):\n",
    "    frame = result.orig_img\n",
    "\n",
    "    if result.boxes.id is None:\n",
    "        cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "        if cv.waitKey(1) == ord('q'): break\n",
    "        continue\n",
    "\n",
    "    boxes = result.boxes.xyxy.cpu().numpy()\n",
    "    tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "    for box, tracker_id in zip(boxes, tracker_ids):\n",
    "        final_id = -1\n",
    "        \n",
    "        if tracker_id in tracker_to_reid:\n",
    "            final_id = tracker_to_reid[tracker_id]\n",
    "        else:\n",
    "            # --- This is the new, corrected block ---\n",
    "            # This is a NEW track, so we perform Re-ID\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            crop = frame[y1:y2, x1:x2]\n",
    "            if crop.size == 0: continue\n",
    "\n",
    "            # 1. Convert NumPy crop to a PyTorch tensor with the correct shape (B, C, H, W)\n",
    "           # Apply the correct transform (resize + normalize)\n",
    "            processed_crop = transform(crop)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                features = loaded_model(processed_crop)\n",
    "                # features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "            \n",
    "            # --- Matching Logic ---\n",
    "            best_match_id = -1\n",
    "            max_similarity = -1\n",
    "\n",
    "            for pid, stored_feature in tracked_identities.items():\n",
    "                similarity = torch.nn.functional.cosine_similarity(features, stored_feature).item()\n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_match_id = pid\n",
    "            \n",
    "            if max_similarity > similarity_threshold:\n",
    "                final_id = best_match_id\n",
    "                # Optional: Update the stored feature with the new one to adapt to appearance changes\n",
    "                tracked_identities[final_id] = (tracked_identities[final_id]+features)/2\n",
    "            else:\n",
    "                final_id = next_permanent_id\n",
    "                tracked_identities[final_id] = features\n",
    "                next_permanent_id += 1\n",
    "            print(len(tracked_identities),tracked_identities)\n",
    "            tracker_to_reid[tracker_id] = final_id\n",
    "            \n",
    "        # --- Drawing Logic ---\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = f\"ID: {final_id}\"\n",
    "        cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv.putText(frame, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    # --- FPS Calculation ---\n",
    "    ctime = time.time()\n",
    "    if ptime != 0:\n",
    "        fps = 1 / (ctime - ptime)\n",
    "        cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    ptime = ctime\n",
    "\n",
    "    cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4bee518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {0: deque([tensor([-1.0033, -0.9493,  0.3245,  ..., -0.1539,  1.0188, -0.0689], device='cuda:0'),\n",
       "                    tensor([-0.9675, -0.8747,  0.6253,  ..., -0.0103,  0.9891, -0.1327], device='cuda:0'),\n",
       "                    tensor([-0.7900, -1.0234,  0.5660,  ..., -0.1318,  1.3328, -0.1080], device='cuda:0'),\n",
       "                    tensor([-0.7561, -0.9820,  0.2295,  ..., -0.2123,  0.3549, -0.0228], device='cuda:0'),\n",
       "                    tensor([-0.6940, -1.0054,  0.0840,  ..., -0.1727,  1.2262,  0.4132], device='cuda:0'),\n",
       "                    tensor([-0.9026, -0.9291,  0.3671,  ..., -0.1831,  0.9804, -0.0474], device='cuda:0'),\n",
       "                    tensor([-0.8541, -0.9756,  0.5649,  ..., -0.1509,  0.7687,  0.3353], device='cuda:0'),\n",
       "                    tensor([-0.6701, -0.9548,  0.4814,  ..., -0.0406,  1.5448,  0.0554], device='cuda:0'),\n",
       "                    tensor([-0.6420, -0.9864,  0.5883,  ..., -0.0242,  1.7718, -0.0307], device='cuda:0'),\n",
       "                    tensor([-0.7964, -1.0179,  0.0679,  ..., -0.1971,  1.3416,  0.3563], device='cuda:0'),\n",
       "                    tensor([-0.5629, -1.0386,  0.4369,  ..., -0.1427,  1.0195, -0.1998], device='cuda:0'),\n",
       "                    tensor([-0.6492, -0.9808,  0.5866,  ..., -0.0767,  1.5451,  0.0555], device='cuda:0'),\n",
       "                    tensor([-0.7016, -0.9745, -0.0091,  ..., -0.1674,  1.1819,  0.4261], device='cuda:0'),\n",
       "                    tensor([-0.5514, -1.0053,  0.3149,  ..., -0.2056,  0.5195,  0.1933], device='cuda:0'),\n",
       "                    tensor([-0.6886, -0.9831,  0.5120,  ..., -0.1241,  1.3744,  0.0852], device='cuda:0'),\n",
       "                    tensor([-0.8013, -0.9891,  0.0578,  ..., -0.1512,  1.3359,  0.3324], device='cuda:0'),\n",
       "                    tensor([-0.8413, -1.0204,  0.6576,  ..., -0.1033,  0.8370,  0.1932], device='cuda:0'),\n",
       "                    tensor([-0.6200, -1.0389,  0.3453,  ..., -0.1954,  0.8457,  0.3553], device='cuda:0'),\n",
       "                    tensor([-0.7333, -1.0534,  0.4583,  ..., -0.1487,  1.1942,  0.0028], device='cuda:0')],\n",
       "                   maxlen=30),\n",
       "             1: deque([tensor([-1.0182, -0.8984,  0.3412,  ..., -0.1794,  0.5851,  0.0252], device='cuda:0'),\n",
       "                    tensor([-0.8580, -0.9663,  0.2549,  ..., -0.2198,  0.3827,  0.0324], device='cuda:0')],\n",
       "                   maxlen=30)})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracked_identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abfc5b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cosine_similarity(): argument 'x2' (position 2) must be Tensor, not collections.deque",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracked_identities\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracked_identities\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: cosine_similarity(): argument 'x2' (position 2) must be Tensor, not collections.deque"
     ]
    }
   ],
   "source": [
    "torch.nn.functional.cosine_similarity(tracked_identities[0][0], tracked_identities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3dad3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "0: 480x640 1 person, 1285.7ms\n",
      "0: 480x640 1 person, 8.2ms\n",
      "0: 480x640 1 person, 9.9ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 7.1ms\n",
      "0: 480x640 1 person, 9.1ms\n",
      "0: 480x640 1 person, 6.8ms\n",
      "0: 480x640 1 person, 6.4ms\n",
      "0: 480x640 1 person, 7.6ms\n",
      "0: 480x640 1 person, 7.0ms\n",
      "0: 480x640 1 person, 7.2ms\n",
      "0: 480x640 1 person, 8.1ms\n",
      "0: 480x640 1 person, 6.6ms\n",
      "0: 480x640 1 person, 6.6ms\n",
      "0: 480x640 1 person, 7.2ms\n",
      "0: 480x640 1 person, 9.4ms\n",
      "0: 480x640 1 person, 6.8ms\n",
      "0: 480x640 1 person, 7.2ms\n",
      "0: 480x640 1 person, 6.4ms\n",
      "0: 480x640 1 person, 8.7ms\n",
      "0: 480x640 1 person, 7.1ms\n",
      "0: 480x640 1 person, 6.6ms\n",
      "0: 480x640 1 person, 9.3ms\n",
      "0: 480x640 1 person, 8.2ms\n",
      "0: 480x640 1 person, 11.4ms\n",
      "0: 480x640 1 person, 7.7ms\n",
      "0: 480x640 1 person, 7.6ms\n",
      "0: 480x640 1 person, 7.6ms\n",
      "0: 480x640 1 person, 7.1ms\n",
      "0: 480x640 1 person, 7.4ms\n",
      "0: 480x640 1 person, 7.6ms\n",
      "0: 480x640 1 person, 7.3ms\n",
      "0: 480x640 1 person, 7.4ms\n",
      "0: 480x640 1 person, 8.8ms\n",
      "0: 480x640 1 person, 6.9ms\n",
      "0: 480x640 1 person, 9.0ms\n",
      "0: 480x640 1 person, 7.3ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 6.7ms\n",
      "0: 480x640 1 person, 6.9ms\n",
      "0: 480x640 1 person, 6.4ms\n",
      "0: 480x640 1 person, 6.6ms\n",
      "0: 480x640 1 person, 7.5ms\n",
      "0: 480x640 1 person, 7.0ms\n",
      "0: 480x640 1 person, 6.8ms\n",
      "0: 480x640 1 person, 8.3ms\n",
      "0: 480x640 1 person, 7.0ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 7.3ms\n",
      "0: 480x640 1 person, 8.7ms\n",
      "0: 480x640 1 person, 9.3ms\n",
      "0: 480x640 1 person, 9.5ms\n",
      "0: 480x640 1 person, 9.1ms\n",
      "0: 480x640 1 person, 9.1ms\n",
      "0: 480x640 1 person, 10.3ms\n",
      "0: 480x640 1 person, 9.4ms\n",
      "0: 480x640 1 person, 9.7ms\n",
      "0: 480x640 1 person, 9.9ms\n",
      "0: 480x640 1 person, 9.4ms\n",
      "0: 480x640 1 person, 10.3ms\n",
      "0: 480x640 1 person, 10.7ms\n",
      "0: 480x640 1 person, 10.1ms\n",
      "0: 480x640 1 person, 9.6ms\n",
      "0: 480x640 1 person, 9.9ms\n",
      "0: 480x640 1 person, 10.3ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 1 person, 9.9ms\n",
      "0: 480x640 1 person, 10.7ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "0: 480x640 1 person, 10.4ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 10.7ms\n",
      "0: 480x640 1 person, 12.3ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 10.7ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 11.7ms\n",
      "0: 480x640 1 person, 12.2ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 1 person, 12.6ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 1 person, 11.3ms\n",
      "0: 480x640 1 person, 11.3ms\n",
      "0: 480x640 1 person, 11.7ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 13.1ms\n",
      "0: 480x640 1 person, 13.6ms\n",
      "0: 480x640 1 person, 12.8ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 3 persons, 11.5ms\n",
      "0: 480x640 3 persons, 12.1ms\n",
      "0: 480x640 3 persons, 11.9ms\n",
      "0: 480x640 3 persons, 11.4ms\n",
      "0: 480x640 3 persons, 11.8ms\n",
      "0: 480x640 3 persons, 11.7ms\n",
      "0: 480x640 3 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 3 persons, 10.5ms\n",
      "0: 480x640 3 persons, 9.7ms\n",
      "0: 480x640 3 persons, 10.0ms\n",
      "0: 480x640 3 persons, 9.9ms\n",
      "0: 480x640 3 persons, 10.1ms\n",
      "0: 480x640 3 persons, 9.6ms\n",
      "0: 480x640 3 persons, 9.9ms\n",
      "0: 480x640 3 persons, 10.7ms\n",
      "0: 480x640 3 persons, 9.7ms\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 9.8ms\n",
      "0: 480x640 3 persons, 10.8ms\n",
      "0: 480x640 3 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 9.6ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 13.0ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.7ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.1ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.1ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.7ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 3 persons, 10.1ms\n",
      "0: 480x640 3 persons, 10.2ms\n",
      "0: 480x640 3 persons, 10.3ms\n",
      "0: 480x640 3 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.7ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 9.9ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 3 persons, 9.9ms\n",
      "0: 480x640 3 persons, 9.9ms\n",
      "0: 480x640 3 persons, 10.4ms\n",
      "0: 480x640 3 persons, 11.1ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 1 person, 11.7ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 11.4ms\n",
      "0: 480x640 1 person, 11.8ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 12.1ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 12.6ms\n",
      "0: 480x640 1 person, 12.9ms\n",
      "0: 480x640 1 person, 13.9ms\n",
      "0: 480x640 1 person, 12.8ms\n",
      "0: 480x640 1 person, 12.5ms\n",
      "0: 480x640 1 person, 12.5ms\n",
      "0: 480x640 1 person, 14.0ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 13.0ms\n",
      "0: 480x640 1 person, 12.6ms\n",
      "0: 480x640 1 person, 12.5ms\n",
      "0: 480x640 1 person, 13.2ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 13.7ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 12.6ms\n",
      "0: 480x640 1 person, 13.7ms\n",
      "0: 480x640 1 person, 14.0ms\n",
      "0: 480x640 1 person, 13.3ms\n",
      "0: 480x640 1 person, 13.9ms\n",
      "0: 480x640 1 person, 12.5ms\n",
      "0: 480x640 1 person, 12.5ms\n",
      "0: 480x640 1 person, 12.5ms\n",
      "0: 480x640 1 person, 12.6ms\n",
      "0: 480x640 1 person, 13.1ms\n",
      "0: 480x640 1 person, 13.3ms\n",
      "0: 480x640 1 person, 13.4ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 12.1ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 13.1ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 11.8ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 12.4ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 11.3ms\n",
      "0: 480x640 1 person, 11.8ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 12.1ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.1ms\n",
      "0: 480x640 3 persons, 11.2ms\n",
      "0: 480x640 3 persons, 12.2ms\n",
      "0: 480x640 3 persons, 10.1ms\n",
      "0: 480x640 3 persons, 10.1ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 3 persons, 10.3ms\n",
      "0: 480x640 3 persons, 10.7ms\n",
      "0: 480x640 3 persons, 10.4ms\n",
      "0: 480x640 3 persons, 10.5ms\n",
      "0: 480x640 3 persons, 10.5ms\n",
      "0: 480x640 3 persons, 10.6ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.3ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.7ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.7ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.1ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 3 persons, 11.5ms\n",
      "0: 480x640 3 persons, 11.5ms\n",
      "0: 480x640 3 persons, 11.6ms\n",
      "0: 480x640 3 persons, 11.5ms\n",
      "0: 480x640 3 persons, 11.5ms\n",
      "0: 480x640 3 persons, 12.5ms\n",
      "0: 480x640 3 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 3 persons, 11.8ms\n",
      "0: 480x640 3 persons, 13.3ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 1 person, 11.7ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 14.6ms\n",
      "0: 480x640 1 person, 13.2ms\n",
      "0: 480x640 1 person, 13.2ms\n",
      "0: 480x640 1 person, 13.3ms\n",
      "0: 480x640 1 person, 14.5ms\n",
      "0: 480x640 1 person, 14.5ms\n",
      "0: 480x640 1 person, 13.0ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 12.2ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 12.6ms\n",
      "0: 480x640 1 person, 13.1ms\n",
      "0: 480x640 1 person, 13.0ms\n",
      "0: 480x640 1 person, 13.2ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 11.3ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 11.3ms\n",
      "0: 480x640 3 persons, 10.8ms\n",
      "0: 480x640 3 persons, 11.1ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 11.3ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 11.3ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 11.3ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 11.3ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 12.1ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 13.7ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 13.0ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 13.1ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 12.1ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 13.0ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 13.8ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 13.1ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 3 persons, 10.9ms\n",
      "0: 480x640 3 persons, 10.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.1ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 3 persons, 11.1ms\n",
      "0: 480x640 3 persons, 10.9ms\n",
      "0: 480x640 3 persons, 11.6ms\n",
      "0: 480x640 3 persons, 11.2ms\n",
      "0: 480x640 3 persons, 10.6ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 2 persons, 11.3ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 1 person, 10.3ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 10.3ms\n",
      "0: 480x640 1 person, 10.4ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 1 person, 11.3ms\n",
      "0: 480x640 1 person, 10.4ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 1 person, 10.0ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 13.9ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.7ms\n",
      "0: 480x640 2 persons, 11.1ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 10.0ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.7ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 10.4ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.8ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 3 persons, 11.8ms\n",
      "0: 480x640 3 persons, 10.8ms\n",
      "0: 480x640 3 persons, 10.8ms\n",
      "0: 480x640 3 persons, 11.4ms\n",
      "0: 480x640 3 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 11.2ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 11.0ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 11.5ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 13.0ms\n",
      "0: 480x640 2 persons, 13.3ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 12.1ms\n",
      "0: 480x640 2 persons, 11.8ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 13.4ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.1ms\n",
      "0: 480x640 2 persons, 13.7ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 13.6ms\n",
      "0: 480x640 2 persons, 13.5ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "WARNING Waiting for stream 0\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 14.5ms\n",
      "0: 480x640 2 persons, 13.5ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 14.0ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 13.6ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 13.3ms\n",
      "0: 480x640 2 persons, 13.3ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 13.2ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 13.4ms\n",
      "0: 480x640 1 person, 13.2ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 10.1ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "0: 480x640 1 person, 10.3ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.4ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 10.4ms\n",
      "0: 480x640 1 person, 10.4ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 10.3ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 9 persons, 10.2ms\n",
      "0: 480x640 1 person, 10.7ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "0: 480x640 2 persons, 10.2ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 11.4ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.5ms\n",
      "0: 480x640 2 persons, 11.7ms\n",
      "0: 480x640 2 persons, 11.6ms\n",
      "0: 480x640 2 persons, 10.3ms\n",
      "0: 480x640 2 persons, 10.6ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.7ms\n",
      "0: 480x640 2 persons, 12.3ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 14.3ms\n",
      "0: 480x640 2 persons, 11.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.0ms\n",
      "0: 480x640 2 persons, 12.1ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 2 persons, 12.1ms\n",
      "0: 480x640 2 persons, 12.8ms\n",
      "0: 480x640 2 persons, 14.0ms\n",
      "0: 480x640 2 persons, 13.5ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 13.5ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 12.4ms\n",
      "0: 480x640 2 persons, 13.6ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 12.6ms\n",
      "0: 480x640 2 persons, 12.5ms\n",
      "0: 480x640 2 persons, 13.0ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 14.3ms\n",
      "0: 480x640 2 persons, 13.8ms\n",
      "0: 480x640 2 persons, 13.1ms\n",
      "0: 480x640 2 persons, 12.9ms\n",
      "0: 480x640 2 persons, 12.2ms\n",
      "0: 480x640 1 person, 11.3ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 4 persons, 10.8ms\n",
      "0: 480x640 3 persons, 12.1ms\n",
      "0: 480x640 2 persons, 10.9ms\n",
      "0: 480x640 1 person, 11.9ms\n",
      "0: 480x640 1 person, 12.9ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 11.7ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 12.1ms\n",
      "0: 480x640 1 person, 12.0ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 11.3ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 12.2ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 12.1ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 11.5ms\n",
      "0: 480x640 1 person, 11.2ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 11.6ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 11.1ms\n",
      "0: 480x640 1 person, 12.1ms\n",
      "0: 480x640 1 person, 10.8ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 11.8ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 10.9ms\n",
      "0: 480x640 1 person, 11.0ms\n",
      "0: 480x640 1 person, 10.6ms\n",
      "0: 480x640 1 person, 10.2ms\n",
      "0: 480x640 1 person, 10.5ms\n",
      "0: 480x640 1 person, 11.1ms\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import torch\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Preprocessing for ReID ---\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "SIMILARITY_THRESHOLD = 0.92     # ReID cosine similarity threshold\n",
    "FEATURE_HISTORY = 30           # Keep last N embeddings per ID\n",
    "CONFIRM_FRAMES = 8             # Frames before promoting a new ID\n",
    "MIN_BOX_AREA = 800              # Ignore tiny detections\n",
    "pending_tracks = defaultdict(lambda: {\"count\": 0, \"feature\": None})\n",
    "\n",
    "# --- Data structures ---\n",
    "tracked_identities = defaultdict(lambda: deque(maxlen=FEATURE_HISTORY))  # Permanent IDs -> deque of features\n",
    "tracker_to_reid = {}    # ByteTrack ID -> permanent ID\n",
    "next_permanent_id = 0\n",
    "\n",
    "# --- Load YOLO model ---\n",
    "# model3 = YOLO(\"yolov8n.pt\")  # Replace with your trained weights\n",
    "\n",
    "# --- Load FastReID model ---\n",
    "\n",
    "\n",
    "# --- Function: Cosine similarity between new feature and stored ones ---\n",
    "# def best_match(feature, stored_features):\n",
    "#     best_id = -1\n",
    "#     best_sim = -1\n",
    "#     for pid, feats in stored_features.items():\n",
    "#         feats_tensor = torch.stack(list(feats))\n",
    "#         sims = torch.nn.functional.cosine_similarity(feature, feats_tensor)\n",
    "#         max_sim = sims.max().item()\n",
    "        \n",
    "#         if max_sim > best_sim:\n",
    "#             best_sim = max_sim\n",
    "#             best_id = pid\n",
    "#     return best_id, best_sim\n",
    "def best_match(feature, stored_features):\n",
    "    best_id = -1\n",
    "    best_score = -1\n",
    "\n",
    "    for pid, feats in stored_features.items():\n",
    "        feats_tensor = torch.stack(list(feats))\n",
    "        \n",
    "        # Max similarity\n",
    "        max_sim = torch.nn.functional.cosine_similarity(feature, feats_tensor).max().item()\n",
    "        \n",
    "        # Average similarity\n",
    "        avg_feat = feats_tensor.mean(dim=0, keepdim=True)\n",
    "        avg_sim = torch.nn.functional.cosine_similarity(feature, avg_feat).item()\n",
    "        \n",
    "        # Weighted score\n",
    "        final_score = 0.85 * max_sim + 0.15 * avg_sim\n",
    "\n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            best_id = pid\n",
    "\n",
    "    return best_id, best_score\n",
    "\n",
    "# --- Function: Preprocess crop for ReID ---\n",
    "def preprocess_crop(frame, box):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    crop = frame[y1:y2, x1:x2]\n",
    "    if crop.size == 0 or (x2-x1)*(y2-y1) < MIN_BOX_AREA:\n",
    "        return None\n",
    "    return transform(crop)\n",
    "\n",
    "# --- Main loop ---\n",
    "ptime = 0\n",
    "for result in model3.track(source=0, stream=True, tracker=\"bytetrack.yaml\", classes=[0]):\n",
    "    frame = result.orig_img\n",
    "\n",
    "    if result.boxes.id is None:\n",
    "        cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    boxes = result.boxes.xyxy.cpu().numpy()\n",
    "    tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "    crops_to_match = []\n",
    "    id_mapping = []\n",
    "\n",
    "    # Step 1: Collect crops for unmatched tracker IDs\n",
    "    for box, tracker_id in zip(boxes, tracker_ids):\n",
    "        if tracker_id not in tracker_to_reid:\n",
    "            crop_tensor = preprocess_crop(frame, box)\n",
    "            if crop_tensor is not None:\n",
    "                crops_to_match.append(crop_tensor)\n",
    "                id_mapping.append(tracker_id)\n",
    "\n",
    "    # Step 2: Batch ReID for new/unmatched tracker IDs\n",
    "    if crops_to_match:\n",
    "        batch_tensor = torch.cat(crops_to_match, dim=0)\n",
    "        with torch.no_grad():\n",
    "            features_batch = loaded_model(batch_tensor)\n",
    "\n",
    "        for tracker_id, feature in zip(id_mapping, features_batch):\n",
    "            best_id, sim = best_match(feature, tracked_identities)\n",
    "\n",
    "            if sim > SIMILARITY_THRESHOLD:\n",
    "                # Match found\n",
    "                tracker_to_reid[tracker_id] = best_id\n",
    "                tracked_identities[best_id].append(feature)\n",
    "            else:\n",
    "                # New track confirmation system\n",
    "                pending_tracks[tracker_id][\"count\"] += 1\n",
    "                if pending_tracks[tracker_id][\"count\"] >= CONFIRM_FRAMES:\n",
    "                    global next_permanent_id\n",
    "                    new_id = next_permanent_id\n",
    "                    next_permanent_id += 1\n",
    "                    tracker_to_reid[tracker_id] = new_id\n",
    "                    tracked_identities[new_id].append(feature)\n",
    "                    del pending_tracks[tracker_id]\n",
    "                else:\n",
    "                    pending_tracks[tracker_id][\"feature\"] = feature\n",
    "\n",
    "    # Step 3: Drawing\n",
    "    for box, tracker_id in zip(boxes, tracker_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        if tracker_id in tracker_to_reid:\n",
    "            final_id = tracker_to_reid[tracker_id]\n",
    "        else:\n",
    "            final_id = -1  # Pending\n",
    "\n",
    "        label = f\"ID: {final_id}\" if final_id != -1 else \"Pending\"\n",
    "        cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv.putText(frame, label, (x1, y1 - 10),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    # Step 4: FPS counter\n",
    "    ctime = time.time()\n",
    "    if ptime != 0:\n",
    "        fps = 1 / (ctime - ptime)\n",
    "        cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    ptime = ctime\n",
    "\n",
    "    # Step 5: Show\n",
    "    cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19505480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "video 1/1 (frame 1/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 206.4ms\n",
      "video 1/1 (frame 2/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.9ms\n",
      "video 1/1 (frame 3/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.4ms\n",
      "video 1/1 (frame 4/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.1ms\n",
      "video 1/1 (frame 5/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.3ms\n",
      "video 1/1 (frame 6/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 7/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 6.9ms\n",
      "video 1/1 (frame 8/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 6.8ms\n",
      "[01:55:56] Person 0 appeared.\n",
      "[01:55:56] Person 1 appeared.\n",
      "[01:55:56] Person 2 appeared.\n",
      "[01:55:56] Person 3 appeared.\n",
      "[01:55:56] Person 4 appeared.\n",
      "[01:55:56] Person 5 appeared.\n",
      "[01:55:56] Person 6 appeared.\n",
      "[01:55:56] Person 7 appeared.\n",
      "[01:55:56] Person 8 appeared.\n",
      "[01:55:56] Person 9 appeared.\n",
      "video 1/1 (frame 9/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 7.5ms\n",
      "video 1/1 (frame 10/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.2ms\n",
      "video 1/1 (frame 11/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.5ms\n",
      "video 1/1 (frame 12/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 13/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.0ms\n",
      "video 1/1 (frame 14/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.4ms\n",
      "video 1/1 (frame 15/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 16/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 12.4ms\n",
      "video 1/1 (frame 17/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.4ms\n",
      "[01:55:57] Person 10 appeared.\n",
      "[01:55:57] Person 11 appeared.\n",
      "video 1/1 (frame 18/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.7ms\n",
      "video 1/1 (frame 19/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 6.5ms\n",
      "video 1/1 (frame 20/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 21/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.1ms\n",
      "video 1/1 (frame 22/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 11.3ms\n",
      "video 1/1 (frame 23/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.0ms\n",
      "video 1/1 (frame 24/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.3ms\n",
      "video 1/1 (frame 25/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 26/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 6.1ms\n",
      "video 1/1 (frame 27/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 28/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 6.8ms\n",
      "video 1/1 (frame 29/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 11.8ms\n",
      "video 1/1 (frame 30/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.3ms\n",
      "video 1/1 (frame 31/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.9ms\n",
      "video 1/1 (frame 32/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 33/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 34/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.9ms\n",
      "video 1/1 (frame 35/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.9ms\n",
      "video 1/1 (frame 36/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.8ms\n",
      "video 1/1 (frame 37/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 38/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.8ms\n",
      "[01:55:57] Person 12 appeared.\n",
      "video 1/1 (frame 39/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.3ms\n",
      "video 1/1 (frame 40/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.4ms\n",
      "video 1/1 (frame 41/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.8ms\n",
      "video 1/1 (frame 42/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.0ms\n",
      "video 1/1 (frame 43/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.5ms\n",
      "video 1/1 (frame 44/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.9ms\n",
      "video 1/1 (frame 45/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.2ms\n",
      "video 1/1 (frame 46/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 47/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 48/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.6ms\n",
      "video 1/1 (frame 49/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.3ms\n",
      "video 1/1 (frame 50/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.7ms\n",
      "video 1/1 (frame 51/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.3ms\n",
      "video 1/1 (frame 52/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.2ms\n",
      "video 1/1 (frame 53/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.6ms\n",
      "video 1/1 (frame 54/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.7ms\n",
      "video 1/1 (frame 55/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 56/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 57/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 58/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.7ms\n",
      "video 1/1 (frame 59/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 60/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.4ms\n",
      "video 1/1 (frame 61/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 6.7ms\n",
      "video 1/1 (frame 62/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 63/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 5.8ms\n",
      "video 1/1 (frame 64/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 5.7ms\n",
      "video 1/1 (frame 65/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 6.8ms\n",
      "video 1/1 (frame 66/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.0ms\n",
      "video 1/1 (frame 67/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.1ms\n",
      "video 1/1 (frame 68/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.2ms\n",
      "video 1/1 (frame 69/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.4ms\n",
      "video 1/1 (frame 70/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.6ms\n",
      "[01:55:58] Person 13 appeared.\n",
      "video 1/1 (frame 71/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.1ms\n",
      "video 1/1 (frame 72/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.2ms\n",
      "video 1/1 (frame 73/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.1ms\n",
      "video 1/1 (frame 74/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.0ms\n",
      "video 1/1 (frame 75/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.9ms\n",
      "video 1/1 (frame 76/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.9ms\n",
      "video 1/1 (frame 77/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.3ms\n",
      "video 1/1 (frame 78/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.3ms\n",
      "video 1/1 (frame 79/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.9ms\n",
      "video 1/1 (frame 80/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.2ms\n",
      "video 1/1 (frame 81/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.6ms\n",
      "[01:55:59] Person 14 appeared.\n",
      "video 1/1 (frame 82/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.4ms\n",
      "video 1/1 (frame 83/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 10.5ms\n",
      "video 1/1 (frame 84/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.7ms\n",
      "video 1/1 (frame 85/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 10.4ms\n",
      "[01:55:59] Person 15 appeared.\n",
      "video 1/1 (frame 86/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.5ms\n",
      "video 1/1 (frame 87/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.5ms\n",
      "video 1/1 (frame 88/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.7ms\n",
      "video 1/1 (frame 89/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.8ms\n",
      "video 1/1 (frame 90/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 91/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.5ms\n",
      "video 1/1 (frame 92/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.6ms\n",
      "video 1/1 (frame 93/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.8ms\n",
      "video 1/1 (frame 94/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 10.5ms\n",
      "video 1/1 (frame 95/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.6ms\n",
      "video 1/1 (frame 96/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.4ms\n",
      "video 1/1 (frame 97/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.5ms\n",
      "video 1/1 (frame 98/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.6ms\n",
      "video 1/1 (frame 99/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.2ms\n",
      "video 1/1 (frame 100/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.8ms\n",
      "video 1/1 (frame 101/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 12.8ms\n",
      "video 1/1 (frame 102/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.6ms\n",
      "video 1/1 (frame 103/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.8ms\n",
      "video 1/1 (frame 104/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.1ms\n",
      "video 1/1 (frame 105/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.6ms\n",
      "video 1/1 (frame 106/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.3ms\n",
      "video 1/1 (frame 107/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.5ms\n",
      "video 1/1 (frame 108/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.5ms\n",
      "video 1/1 (frame 109/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.6ms\n",
      "video 1/1 (frame 110/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.2ms\n",
      "video 1/1 (frame 111/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.8ms\n",
      "video 1/1 (frame 112/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.9ms\n",
      "video 1/1 (frame 113/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 7.6ms\n",
      "video 1/1 (frame 114/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.4ms\n",
      "video 1/1 (frame 115/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 116/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 117/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.3ms\n",
      "video 1/1 (frame 118/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 119/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.1ms\n",
      "video 1/1 (frame 120/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.8ms\n",
      "video 1/1 (frame 121/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.4ms\n",
      "video 1/1 (frame 122/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.3ms\n",
      "video 1/1 (frame 123/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 124/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.3ms\n",
      "video 1/1 (frame 125/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.2ms\n",
      "video 1/1 (frame 126/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.7ms\n",
      "video 1/1 (frame 127/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 128/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.6ms\n",
      "video 1/1 (frame 129/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.2ms\n",
      "video 1/1 (frame 130/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.7ms\n",
      "video 1/1 (frame 131/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 7.8ms\n",
      "video 1/1 (frame 132/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 7.7ms\n",
      "video 1/1 (frame 133/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.3ms\n",
      "video 1/1 (frame 134/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.5ms\n",
      "video 1/1 (frame 135/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.8ms\n",
      "video 1/1 (frame 136/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.8ms\n",
      "video 1/1 (frame 137/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.3ms\n",
      "video 1/1 (frame 138/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.0ms\n",
      "video 1/1 (frame 139/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.5ms\n",
      "video 1/1 (frame 140/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.3ms\n",
      "video 1/1 (frame 141/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.8ms\n",
      "video 1/1 (frame 142/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.9ms\n",
      "video 1/1 (frame 143/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 144/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.4ms\n",
      "video 1/1 (frame 145/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.4ms\n",
      "video 1/1 (frame 146/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.0ms\n",
      "video 1/1 (frame 147/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 7.9ms\n",
      "video 1/1 (frame 148/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.3ms\n",
      "video 1/1 (frame 149/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 150/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.2ms\n",
      "video 1/1 (frame 151/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.8ms\n",
      "video 1/1 (frame 152/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 153/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 154/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.3ms\n",
      "video 1/1 (frame 155/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.2ms\n",
      "video 1/1 (frame 156/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.5ms\n",
      "video 1/1 (frame 157/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.4ms\n",
      "video 1/1 (frame 158/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 159/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 160/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.1ms\n",
      "video 1/1 (frame 161/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 162/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.8ms\n",
      "video 1/1 (frame 163/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 164/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.5ms\n",
      "video 1/1 (frame 165/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.2ms\n",
      "video 1/1 (frame 166/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.2ms\n",
      "video 1/1 (frame 167/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.3ms\n",
      "video 1/1 (frame 168/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.5ms\n",
      "video 1/1 (frame 169/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.5ms\n",
      "video 1/1 (frame 170/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "[01:56:01] Person 12 reappeared after 3.4 sec.\n",
      "video 1/1 (frame 171/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.4ms\n",
      "video 1/1 (frame 172/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 173/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.1ms\n",
      "video 1/1 (frame 174/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 175/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.5ms\n",
      "video 1/1 (frame 176/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "[01:56:01] Person 16 appeared.\n",
      "video 1/1 (frame 177/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.3ms\n",
      "video 1/1 (frame 178/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.3ms\n",
      "video 1/1 (frame 179/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.1ms\n",
      "video 1/1 (frame 180/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 181/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.2ms\n",
      "video 1/1 (frame 182/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.7ms\n",
      "video 1/1 (frame 183/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.2ms\n",
      "video 1/1 (frame 184/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.9ms\n",
      "video 1/1 (frame 185/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 186/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 187/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.3ms\n",
      "video 1/1 (frame 188/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 189/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.6ms\n",
      "video 1/1 (frame 190/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.2ms\n",
      "video 1/1 (frame 191/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.5ms\n",
      "video 1/1 (frame 192/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.6ms\n",
      "[01:56:02] Person 17 appeared.\n",
      "video 1/1 (frame 193/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.2ms\n",
      "video 1/1 (frame 194/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.5ms\n",
      "video 1/1 (frame 195/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 196/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 197/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 198/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 199/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 200/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.5ms\n",
      "video 1/1 (frame 201/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "[01:56:02] Person 18 appeared.\n",
      "video 1/1 (frame 202/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 203/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 204/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 205/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 206/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.1ms\n",
      "video 1/1 (frame 207/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.5ms\n",
      "video 1/1 (frame 208/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.7ms\n",
      "video 1/1 (frame 209/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 210/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.3ms\n",
      "video 1/1 (frame 211/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 212/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 213/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.7ms\n",
      "video 1/1 (frame 214/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 215/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 216/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.1ms\n",
      "video 1/1 (frame 217/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.6ms\n",
      "video 1/1 (frame 218/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.0ms\n",
      "video 1/1 (frame 219/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.6ms\n",
      "video 1/1 (frame 220/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 221/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.1ms\n",
      "video 1/1 (frame 222/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 223/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.6ms\n",
      "video 1/1 (frame 224/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 8.7ms\n",
      "video 1/1 (frame 225/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 226/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 227/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.3ms\n",
      "video 1/1 (frame 228/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.1ms\n",
      "video 1/1 (frame 229/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 11.0ms\n",
      "video 1/1 (frame 230/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.5ms\n",
      "[01:56:03] Person 19 appeared.\n",
      "video 1/1 (frame 231/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 232/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.7ms\n",
      "video 1/1 (frame 233/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.9ms\n",
      "video 1/1 (frame 234/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.6ms\n",
      "video 1/1 (frame 235/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.4ms\n",
      "video 1/1 (frame 236/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 8.9ms\n",
      "video 1/1 (frame 237/329) c:\\CCTV\\palace.mp4: 384x640 14 persons, 9.4ms\n",
      "video 1/1 (frame 238/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.4ms\n",
      "video 1/1 (frame 239/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 11.2ms\n",
      "video 1/1 (frame 240/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.9ms\n",
      "video 1/1 (frame 241/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.2ms\n",
      "[01:56:04] Person 20 appeared.\n",
      "video 1/1 (frame 242/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.3ms\n",
      "video 1/1 (frame 243/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 10.0ms\n",
      "video 1/1 (frame 244/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.6ms\n",
      "video 1/1 (frame 245/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.6ms\n",
      "video 1/1 (frame 246/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.0ms\n",
      "video 1/1 (frame 247/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.0ms\n",
      "video 1/1 (frame 248/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.1ms\n",
      "video 1/1 (frame 249/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.4ms\n",
      "video 1/1 (frame 250/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.0ms\n",
      "video 1/1 (frame 251/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 252/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 10.7ms\n",
      "video 1/1 (frame 253/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.3ms\n",
      "video 1/1 (frame 254/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.8ms\n",
      "video 1/1 (frame 255/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.7ms\n",
      "video 1/1 (frame 256/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.4ms\n",
      "video 1/1 (frame 257/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.6ms\n",
      "video 1/1 (frame 258/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.1ms\n",
      "video 1/1 (frame 259/329) c:\\CCTV\\palace.mp4: 384x640 13 persons, 9.4ms\n",
      "video 1/1 (frame 260/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 261/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.7ms\n",
      "video 1/1 (frame 262/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.5ms\n",
      "video 1/1 (frame 263/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.6ms\n",
      "video 1/1 (frame 264/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 8.8ms\n",
      "video 1/1 (frame 265/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.6ms\n",
      "video 1/1 (frame 266/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.5ms\n",
      "video 1/1 (frame 267/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.2ms\n",
      "video 1/1 (frame 268/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 269/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.1ms\n",
      "video 1/1 (frame 270/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.8ms\n",
      "video 1/1 (frame 271/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 10.0ms\n",
      "video 1/1 (frame 272/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.9ms\n",
      "video 1/1 (frame 273/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.2ms\n",
      "video 1/1 (frame 274/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.0ms\n",
      "video 1/1 (frame 275/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.1ms\n",
      "video 1/1 (frame 276/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.3ms\n",
      "video 1/1 (frame 277/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.3ms\n",
      "video 1/1 (frame 278/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.8ms\n",
      "video 1/1 (frame 279/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.9ms\n",
      "video 1/1 (frame 280/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 281/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 282/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 283/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.3ms\n",
      "video 1/1 (frame 284/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.1ms\n",
      "video 1/1 (frame 285/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.1ms\n",
      "video 1/1 (frame 286/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.3ms\n",
      "video 1/1 (frame 287/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.1ms\n",
      "video 1/1 (frame 288/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.8ms\n",
      "video 1/1 (frame 289/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 290/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.2ms\n",
      "video 1/1 (frame 291/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.4ms\n",
      "video 1/1 (frame 292/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 293/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.8ms\n",
      "video 1/1 (frame 294/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 295/329) c:\\CCTV\\palace.mp4: 384x640 12 persons, 9.3ms\n",
      "[01:56:05] Person 11 reappeared after 4.8 sec.\n",
      "video 1/1 (frame 296/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 10.2ms\n",
      "video 1/1 (frame 297/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.4ms\n",
      "video 1/1 (frame 298/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 8.9ms\n",
      "video 1/1 (frame 299/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.1ms\n",
      "video 1/1 (frame 300/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 301/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.2ms\n",
      "video 1/1 (frame 302/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.9ms\n",
      "video 1/1 (frame 303/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 304/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 305/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 306/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.8ms\n",
      "video 1/1 (frame 307/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.3ms\n",
      "video 1/1 (frame 308/329) c:\\CCTV\\palace.mp4: 384x640 11 persons, 9.7ms\n",
      "video 1/1 (frame 309/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 310/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 311/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.4ms\n",
      "video 1/1 (frame 312/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.3ms\n",
      "video 1/1 (frame 313/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.9ms\n",
      "video 1/1 (frame 314/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 10.1ms\n",
      "video 1/1 (frame 315/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.8ms\n",
      "video 1/1 (frame 316/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.6ms\n",
      "video 1/1 (frame 317/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.6ms\n",
      "video 1/1 (frame 318/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.2ms\n",
      "video 1/1 (frame 319/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.5ms\n",
      "video 1/1 (frame 320/329) c:\\CCTV\\palace.mp4: 384x640 8 persons, 8.1ms\n",
      "video 1/1 (frame 321/329) c:\\CCTV\\palace.mp4: 384x640 8 persons, 8.8ms\n",
      "video 1/1 (frame 322/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 9.2ms\n",
      "video 1/1 (frame 323/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 8.7ms\n",
      "video 1/1 (frame 324/329) c:\\CCTV\\palace.mp4: 384x640 10 persons, 9.0ms\n",
      "video 1/1 (frame 325/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.2ms\n",
      "video 1/1 (frame 326/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 9.0ms\n",
      "video 1/1 (frame 327/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 9.1ms\n",
      "video 1/1 (frame 328/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.2ms\n",
      "video 1/1 (frame 329/329) c:\\CCTV\\palace.mp4: 384x640 9 persons, 8.6ms\n",
      "Speed: 1.1ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import torch\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from collections import deque, defaultdict\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Parameters ---\n",
    "SIMILARITY_THRESHOLD = 0.92\n",
    "FEATURE_HISTORY = 30\n",
    "CONFIRM_FRAMES = 8\n",
    "MIN_BOX_AREA = 800\n",
    "REAPPEAR_THRESHOLD = 2.0  # seconds\n",
    "\n",
    "# --- Data structures ---\n",
    "pending_tracks = defaultdict(lambda: {\"count\": 0, \"feature\": None})\n",
    "tracked_identities = defaultdict(lambda: deque(maxlen=FEATURE_HISTORY))\n",
    "tracker_to_reid = {}\n",
    "next_permanent_id = 0\n",
    "\n",
    "# --- Appearance/reappearance logging structures ---\n",
    "last_seen_time = {}  # permanent_id -> last timestamp\n",
    "appearance_log = defaultdict(list)  # permanent_id -> [(event_type, timestamp)]\n",
    "\n",
    "# --- CSV logging setup ---\n",
    "csv_file_path = os.path.abspath(\"person_appearance_log.csv\")\n",
    "write_header = not os.path.exists(csv_file_path)\n",
    "\n",
    "csv_file = open(csv_file_path, mode=\"a\", newline=\"\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "if write_header:\n",
    "    csv_writer.writerow([\"PersonID\", \"EventType\", \"Time_HHMMSS\"])\n",
    "    csv_file.flush()\n",
    "\n",
    "# --- Best match function ---\n",
    "def best_match(feature, stored_features):\n",
    "    best_id = -1\n",
    "    best_score = -1\n",
    "    for pid, feats in stored_features.items():\n",
    "        feats_tensor = torch.stack(list(feats))\n",
    "        max_sim = torch.nn.functional.cosine_similarity(feature, feats_tensor).max().item()\n",
    "        avg_feat = feats_tensor.mean(dim=0, keepdim=True)\n",
    "        avg_sim = torch.nn.functional.cosine_similarity(feature, avg_feat).item()\n",
    "        final_score = 0.85 * max_sim + 0.15 * avg_sim\n",
    "        if final_score > best_score:\n",
    "            best_score = final_score\n",
    "            best_id = pid\n",
    "    return best_id, best_score\n",
    "\n",
    "# --- Crop preprocessing ---\n",
    "def preprocess_crop(frame, box):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    crop = frame[y1:y2, x1:x2]\n",
    "    if crop.size == 0 or (x2-x1)*(y2-y1) < MIN_BOX_AREA:\n",
    "        return None\n",
    "    return transform(crop)  # Make sure transform is defined\n",
    "\n",
    "# --- Main loop ---\n",
    "ptime = 0\n",
    "for result in model3.track(source=\"palace.mp4\", stream=True, tracker=\"bytetrack.yaml\", classes=[0]):\n",
    "    frame = result.orig_img\n",
    "    current_time = time.time()\n",
    "\n",
    "    if result.boxes.id is None:\n",
    "        cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    boxes = result.boxes.xyxy.cpu().numpy()\n",
    "    tracker_ids = result.boxes.id.cpu().numpy().astype(int)\n",
    "\n",
    "    crops_to_match = []\n",
    "    id_mapping = []\n",
    "\n",
    "    # Step 1: Collect crops for unmatched tracker IDs\n",
    "    for box, tracker_id in zip(boxes, tracker_ids):\n",
    "        if tracker_id not in tracker_to_reid:\n",
    "            crop_tensor = preprocess_crop(frame, box)\n",
    "            if crop_tensor is not None:\n",
    "                crops_to_match.append(crop_tensor)\n",
    "                id_mapping.append(tracker_id)\n",
    "\n",
    "    # Step 2: Batch ReID for new/unmatched tracker IDs\n",
    "    if crops_to_match:\n",
    "        batch_tensor = torch.cat(crops_to_match, dim=0)\n",
    "        with torch.no_grad():\n",
    "            features_batch = loaded_model(batch_tensor)\n",
    "        for tracker_id, feature in zip(id_mapping, features_batch):\n",
    "            best_id, sim = best_match(feature, tracked_identities)\n",
    "            if sim > SIMILARITY_THRESHOLD:\n",
    "                tracker_to_reid[tracker_id] = best_id\n",
    "                tracked_identities[best_id].append(feature)\n",
    "            else:\n",
    "                pending_tracks[tracker_id][\"count\"] += 1\n",
    "                if pending_tracks[tracker_id][\"count\"] >= CONFIRM_FRAMES:\n",
    "                    global next_permanent_id\n",
    "                    new_id = next_permanent_id\n",
    "                    next_permanent_id += 1\n",
    "                    tracker_to_reid[tracker_id] = new_id\n",
    "                    tracked_identities[new_id].append(feature)\n",
    "                    del pending_tracks[tracker_id]\n",
    "                else:\n",
    "                    pending_tracks[tracker_id][\"feature\"] = feature\n",
    "\n",
    "    # Step 3: Drawing + CSV logging\n",
    "    for box, tracker_id in zip(boxes, tracker_ids):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        if tracker_id in tracker_to_reid:\n",
    "            final_id = tracker_to_reid[tracker_id]\n",
    "\n",
    "            # --- Detect appearance/reappearance ---\n",
    "            if final_id not in last_seen_time:\n",
    "                appearance_log[final_id].append((\"appearance\"))\n",
    "                print(f\"[{time.strftime('%H:%M:%S', time.localtime(current_time))}] Person {final_id} appeared.\")\n",
    "                csv_writer.writerow([final_id, \"appearance\",\n",
    "                                     time.strftime(\"%H:%M:%S\", time.localtime(current_time))\n",
    "                                     ])\n",
    "                csv_file.flush()\n",
    "\n",
    "            else:\n",
    "                gap = current_time - last_seen_time[final_id]\n",
    "                if gap > REAPPEAR_THRESHOLD:\n",
    "                    appearance_log[final_id].append((\"reappearance\"))\n",
    "                    print(f\"[{time.strftime('%H:%M:%S', time.localtime(current_time))}] Person {final_id} reappeared after {gap:.1f} sec.\")\n",
    "                    csv_writer.writerow([final_id, \"reappearance\",\n",
    "                                         time.strftime(\"%H:%M:%S\", time.localtime(current_time))\n",
    "                                         ])\n",
    "                    csv_file.flush()\n",
    "\n",
    "            last_seen_time[final_id] = current_time\n",
    "        else:\n",
    "            final_id = -1  # Pending\n",
    "\n",
    "        label = f\"ID: {final_id}\" if final_id != -1 else \"Pending\"\n",
    "        cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv.putText(frame, label, (x1, y1 - 10),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    # Step 4: FPS counter\n",
    "    ctime = time.time()\n",
    "    if ptime != 0:\n",
    "        fps = 1 / (ctime - ptime)\n",
    "        cv.putText(frame, f\"FPS: {int(fps)}\", (10, 30),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    ptime = ctime\n",
    "\n",
    "    # Step 5: Show\n",
    "    cv.imshow(\"Person Re-ID Tracking\", frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- Cleanup ---\n",
    "csv_file.close()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8919ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastreid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
